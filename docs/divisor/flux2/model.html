<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 16.0.0"/>
    <title>divisor.flux2.model API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note{color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.tip{color:#0a3622;background-color:#d1e7dd;border-color:#a3cfbb;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%230a3622%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a2%202%200%200%200-.453-.618A5.98%205.98%200%200%201%202%206m6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201%22/%3E%3C/svg%3E");}.pdoc .alert.important{color:#055160;background-color:#cff4fc;border-color:#9eeaf9;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23055160%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%200a2%202%200%200%200-2%202v12a2%202%200%200%200%202%202h12a2%202%200%200%200%202-2V2a2%202%200%200%200-2-2zm6%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.caution{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M11.46.146A.5.5%200%200%200%2011.107%200H4.893a.5.5%200%200%200-.353.146L.146%204.54A.5.5%200%200%200%200%204.893v6.214a.5.5%200%200%200%20.146.353l4.394%204.394a.5.5%200%200%200%20.353.146h6.214a.5.5%200%200%200%20.353-.146l4.394-4.394a.5.5%200%200%200%20.146-.353V4.893a.5.5%200%200%200-.146-.353zM8%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:27px;vertical-align:bottom;width:50px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../flux2.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;divisor.flux2</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#Flux2Params">Flux2Params</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Flux2Params.__init__">Flux2Params</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.in_channels">in_channels</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.context_in_dim">context_in_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.num_heads">num_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.depth">depth</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.depth_single_blocks">depth_single_blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.axes_dim">axes_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.theta">theta</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2Params.mlp_ratio">mlp_ratio</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Flux2">Flux2</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Flux2.__init__">Flux2</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.in_channels">in_channels</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.out_channels">out_channels</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.num_heads">num_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.pe_embedder">pe_embedder</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.img_in">img_in</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.time_in">time_in</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.guidance_in">guidance_in</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.txt_in">txt_in</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.double_blocks">double_blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.single_blocks">single_blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.double_stream_modulation_img">double_stream_modulation_img</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.double_stream_modulation_txt">double_stream_modulation_txt</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.single_stream_modulation">single_stream_modulation</a>
                        </li>
                        <li>
                                <a class="variable" href="#Flux2.final_layer">final_layer</a>
                        </li>
                        <li>
                                <a class="function" href="#Flux2.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#SelfAttention">SelfAttention</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SelfAttention.__init__">SelfAttention</a>
                        </li>
                        <li>
                                <a class="variable" href="#SelfAttention.num_heads">num_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#SelfAttention.qkv">qkv</a>
                        </li>
                        <li>
                                <a class="variable" href="#SelfAttention.norm">norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#SelfAttention.proj">proj</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#SiLUActivation">SiLUActivation</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SiLUActivation.__init__">SiLUActivation</a>
                        </li>
                        <li>
                                <a class="variable" href="#SiLUActivation.gate_fn">gate_fn</a>
                        </li>
                        <li>
                                <a class="function" href="#SiLUActivation.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Modulation">Modulation</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Modulation.__init__">Modulation</a>
                        </li>
                        <li>
                                <a class="variable" href="#Modulation.is_double">is_double</a>
                        </li>
                        <li>
                                <a class="variable" href="#Modulation.multiplier">multiplier</a>
                        </li>
                        <li>
                                <a class="variable" href="#Modulation.lin">lin</a>
                        </li>
                        <li>
                                <a class="function" href="#Modulation.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#LastLayer">LastLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LastLayer.__init__">LastLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#LastLayer.norm_final">norm_final</a>
                        </li>
                        <li>
                                <a class="variable" href="#LastLayer.linear">linear</a>
                        </li>
                        <li>
                                <a class="variable" href="#LastLayer.adaLN_modulation">adaLN_modulation</a>
                        </li>
                        <li>
                                <a class="function" href="#LastLayer.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#SingleStreamBlock">SingleStreamBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SingleStreamBlock.__init__">SingleStreamBlock</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.hidden_dim">hidden_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.num_heads">num_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.scale">scale</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.mlp_hidden_dim">mlp_hidden_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.mlp_mult_factor">mlp_mult_factor</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.linear1">linear1</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.linear2">linear2</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.norm">norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.pre_norm">pre_norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingleStreamBlock.mlp_act">mlp_act</a>
                        </li>
                        <li>
                                <a class="function" href="#SingleStreamBlock.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#DoubleStreamBlock">DoubleStreamBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DoubleStreamBlock.__init__">DoubleStreamBlock</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.num_heads">num_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.img_norm1">img_norm1</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.mlp_mult_factor">mlp_mult_factor</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.img_attn">img_attn</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.img_norm2">img_norm2</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.img_mlp">img_mlp</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.txt_norm1">txt_norm1</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.txt_attn">txt_attn</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.txt_norm2">txt_norm2</a>
                        </li>
                        <li>
                                <a class="variable" href="#DoubleStreamBlock.txt_mlp">txt_mlp</a>
                        </li>
                        <li>
                                <a class="function" href="#DoubleStreamBlock.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MLPEmbedder">MLPEmbedder</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MLPEmbedder.__init__">MLPEmbedder</a>
                        </li>
                        <li>
                                <a class="variable" href="#MLPEmbedder.in_layer">in_layer</a>
                        </li>
                        <li>
                                <a class="variable" href="#MLPEmbedder.silu">silu</a>
                        </li>
                        <li>
                                <a class="variable" href="#MLPEmbedder.out_layer">out_layer</a>
                        </li>
                        <li>
                                <a class="function" href="#MLPEmbedder.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#EmbedND">EmbedND</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#EmbedND.__init__">EmbedND</a>
                        </li>
                        <li>
                                <a class="variable" href="#EmbedND.dim">dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#EmbedND.theta">theta</a>
                        </li>
                        <li>
                                <a class="variable" href="#EmbedND.axes_dim">axes_dim</a>
                        </li>
                        <li>
                                <a class="function" href="#EmbedND.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#timestep_embedding">timestep_embedding</a>
            </li>
            <li>
                    <a class="class" href="#RMSNorm">RMSNorm</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#RMSNorm.__init__">RMSNorm</a>
                        </li>
                        <li>
                                <a class="variable" href="#RMSNorm.scale">scale</a>
                        </li>
                        <li>
                                <a class="function" href="#RMSNorm.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#QKNorm">QKNorm</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#QKNorm.__init__">QKNorm</a>
                        </li>
                        <li>
                                <a class="variable" href="#QKNorm.query_norm">query_norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#QKNorm.key_norm">key_norm</a>
                        </li>
                        <li>
                                <a class="function" href="#QKNorm.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#attention">attention</a>
            </li>
            <li>
                    <a class="function" href="#rope">rope</a>
            </li>
            <li>
                    <a class="function" href="#apply_rope">apply_rope</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22160%22%20viewBox%3D%220%200%20150%2080%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M132.316%2048.886c.276-4.679%202.342-6.698%204.409-7.982s4.27-1.165%206.751-1.055c1.586.07%203.044.156%204.222-.482%201.142-.619%202.026-1.932%202.162-3.739.268-3.576-1.929-5.368-5.006-5.551s-7.599.524-10.517%201.606c-4.455%201.652-8.588%206.606-9.552%208.992s-2.342%206.193-1.745%2010.873%202.664%209.221%205.878%2011.79%205.878%203.808%2010.103%204.312%203.444.229%206.062.229%205.006-2.202%204.914-4.909-2.296-5.001-4.501-4.863-3.077.505-5.281.229-7.715-2.064-7.899-9.451z%22%20fill%3D%22%23198754%22/%3E%3Ccircle%20cx%3D%22101.504%22%20cy%3D%2248.943%22%20r%3D%2214.208%22%20fill%3D%22none%22%20stroke%3D%22%23198754%22%20stroke-width%3D%229.354%22/%3E%3Cpath%20d%3D%22M87.81.002c-3.637.065-5.001.454-7.014%201.232s-3.443%201.363-6.3%204.282c-1.723%201.76-3.148%205.019-3.776%207.329-.413%201.521-.316%202.63-.316%202.63l-.195%2034.612c.065%205.774-6.755%208.305-9.612%208.37s-9.678-1.038-9.743-9.408%207.128-9.521%208.362-9.521c1.413-.13%202.526-.021%203.718-.016%202.071.009%204.157-.778%204.092-4.671s-4.157-4.736-4.157-4.736c-6.3-.843-11.43%202.206-11.43%202.206S40.917%2038.15%2041.372%2049.634%2051.568%2068.19%2061.311%2068.125s18.316-7.007%2018.445-17.193l.13-22.772c.046-2.291%202.683-3.644%204.476-4.203.745-.232%201.694-.274%201.694-.274l10.457-.13s4.871-.324%207.729-3.114%204.352-6.294%204.352-6.294.974-3.049.13-4.606-.195-1.233-2.792-3.309-8.573-4.477-8.573-4.477S91.447-.063%2087.81.002zM0%2047.169l.065%2028.417S0%2080.127%204.481%2079.997s5.072-3.866%205.049-4.152l-.113-28.482s1.624-7.656%209.937-7.721%2010.002%206.942%2010.002%208.499-.909%2010.51-9.093%2010.51c-.948%200-2.99-.567-4.145-.272-3.919%201-3.194%204.554-3.194%204.554s.065%205.061%207.404%204.996%2018.575-6.034%2018.575-19.074S26.953%2030.04%2019.549%2029.91%201.234%2035.296%200%2047.169z%22%20fill%3D%22%23198754%22/%3E%3Cg%20transform%3D%22matrix%28.325601%200%200%20.325256%20-10.32669%20-45.802786%29%22%3E%3Ccircle%20cx%3D%22297.554%22%20cy%3D%22172.286%22%20r%3D%2216.5%22%20fill%3D%22%23fff%22/%3E%3Cellipse%20cx%3D%22297.709%22%20cy%3D%22172.642%22%20rx%3D%2211.071%22%20ry%3D%2210.871%22%20fill%3D%22%23105a48%22/%3E%3Ccircle%20cx%3D%22304.104%22%20cy%3D%22167.667%22%20r%3D%224.5%22%20fill%3D%22%23fff%22/%3E%3C/g%3E%3Cpath%20d%3D%22M94.661%2017.032l.893-1.476s.99.714%201.916.925%201.575.114%202.955.114l14.565-.162c1.283-.032%203.085-.762%203.02-3.293s-.373-3.503-.373-3.503l1.283-.487s.52.503.877%201.573.309%201.995.292%202.66-.227%201.541-.227%201.541%201.564-.308%202.359-1.038.823-.779%201.489-1.508.812-.86.812-.86.552-.13.877.26.341.957.065%201.46-1.672%202.206-3.247%203.066-2.76%201.427-3.929%201.768-3.848.73-7.063.714l-10.944-.114s-2.143-.081-3.02-.373-2.241-.973-2.598-1.265z%22%20fill%3D%22%23d36d49%22/%3E%3Cg%20fill%3D%22%23105a48%22%3E%3Cellipse%20cx%3D%2293.052%22%20cy%3D%2243.567%22%20rx%3D%22.869%22%20ry%3D%221.014%22%20transform%3D%22rotate%28341.022%29%22/%3E%3Cellipse%20cx%3D%22104.3%22%20cy%3D%22-16.184%22%20rx%3D%22.865%22%20ry%3D%221.009%22%20transform%3D%22rotate%2814.786%29%22/%3E%3C/g%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../../divisor.html">divisor</a><wbr>.<a href="./../flux2.html">flux2</a><wbr>.model    </h1>

                
                        <input id="mod-model-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-model-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="c1"># SPDX-License-Identifier:Apache-2.0</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="c1"># original BFL Flux code from https://github.com/black-forest-labs/flux2</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">einops</span><span class="w"> </span><span class="kn">import</span> <span class="n">rearrange</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">divisor.layer_dropout</span><span class="w"> </span><span class="kn">import</span> <span class="n">process_blocks_with_dropout</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="nd">@dataclass</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Flux2Params</span><span class="p">:</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a>    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a>    <span class="n">context_in_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15360</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a>    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6144</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a>    <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a>    <span class="n">depth_single_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>    <span class="n">axes_dim</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>    <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a>    <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Flux2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Flux2Params</span><span class="p">):</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">in_channels</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">in_channels</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Hidden size </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2"> must be divisible by num_heads </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>        <span class="n">pe_dim</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pe_dim</span><span class="p">:</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="si">}</span><span class="s2"> but expected positional dim </span><span class="si">{</span><span class="n">pe_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span> <span class="o">=</span> <span class="n">EmbedND</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">pe_dim</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">axes_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">)</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">time_in</span> <span class="o">=</span> <span class="n">MLPEmbedder</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">guidance_in</span> <span class="o">=</span> <span class="n">MLPEmbedder</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">context_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a>            <span class="p">[</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a>                <span class="n">DoubleStreamBlock</span><span class="p">(</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a>                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">,</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>                <span class="p">)</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">depth</span><span class="p">)</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>            <span class="p">]</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a>        <span class="p">)</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">single_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a>            <span class="p">[</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>                <span class="n">SingleStreamBlock</span><span class="p">(</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">,</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a>                <span class="p">)</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">depth_single_blocks</span><span class="p">)</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>            <span class="p">]</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>        <span class="p">)</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_img</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>            <span class="n">double</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a>            <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>        <span class="p">)</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_txt</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>            <span class="n">double</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>            <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>        <span class="p">)</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">single_stream_modulation</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">double</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">LastLayer</span><span class="p">(</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>        <span class="p">)</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>        <span class="n">x_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>        <span class="n">timesteps</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>        <span class="n">ctx</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>        <span class="n">ctx_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>        <span class="n">guidance</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>        <span class="n">layer_dropouts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>    <span class="p">):</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>        <span class="n">num_txt_tokens</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>        <span class="n">timestep_emb</span> <span class="o">=</span> <span class="n">timestep_embedding</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>        <span class="n">vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_in</span><span class="p">(</span><span class="n">timestep_emb</span><span class="p">)</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>        <span class="n">guidance_emb</span> <span class="o">=</span> <span class="n">timestep_embedding</span><span class="p">(</span><span class="n">guidance</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>        <span class="n">vec</span> <span class="o">=</span> <span class="n">vec</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">guidance_in</span><span class="p">(</span><span class="n">guidance_emb</span><span class="p">)</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>        <span class="n">double_block_mod_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_img</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>        <span class="n">double_block_mod_txt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_txt</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>        <span class="n">single_block_mod</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_stream_modulation</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_in</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_in</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>        <span class="n">pe_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span><span class="p">(</span><span class="n">x_ids</span><span class="p">)</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>        <span class="n">pe_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span><span class="p">(</span><span class="n">ctx_ids</span><span class="p">)</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>        <span class="n">img</span><span class="p">,</span> <span class="n">txt</span> <span class="o">=</span> <span class="n">process_blocks_with_dropout</span><span class="p">(</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span><span class="p">,</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>            <span class="n">layer_dropouts</span><span class="p">,</span>  <span class="c1"># Would need to be added to flux2 forward signature</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>            <span class="mi">0</span><span class="p">,</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>            <span class="s2">&quot;double&quot;</span><span class="p">,</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>            <span class="k">lambda</span> <span class="n">block</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pe_x</span><span class="p">,</span> <span class="n">pe_ctx</span><span class="p">,</span> <span class="n">double_block_mod_img</span><span class="p">,</span> <span class="n">double_block_mod_txt</span><span class="p">),</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>            <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">txt</span><span class="p">),</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a>        <span class="p">)</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt</span><span class="p">,</span> <span class="n">img</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pe_ctx</span><span class="p">,</span> <span class="n">pe_x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">process_blocks_with_dropout</span><span class="p">(</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">single_blocks</span><span class="p">,</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>            <span class="n">layer_dropouts</span><span class="p">,</span>  <span class="c1"># Would need to be added to flux2 forward signature</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span><span class="p">),</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>            <span class="s2">&quot;single&quot;</span><span class="p">,</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>            <span class="k">lambda</span> <span class="n">block</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="n">single_block_mod</span><span class="p">),</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>            <span class="n">img</span><span class="p">,</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>        <span class="p">)</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="n">num_txt_tokens</span><span class="p">:,</span> <span class="o">...</span><span class="p">]</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>        <span class="k">return</span> <span class="n">img</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>    <span class="p">):</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">QKNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SiLUActivation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x2</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Modulation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">double</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_double</span> <span class="o">=</span> <span class="n">double</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span> <span class="o">=</span> <span class="mi">6</span> <span class="k">if</span> <span class="n">double</span> <span class="k">else</span> <span class="mi">3</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">vec</span><span class="p">))</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>        <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>        <span class="k">return</span> <span class="n">out</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_double</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a><span class="k">class</span><span class="w"> </span><span class="nc">LastLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>    <span class="p">):</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adaLN_modulation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>        <span class="n">mod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adaLN_modulation</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>        <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>        <span class="k">if</span> <span class="n">shift</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>            <span class="n">shift</span> <span class="o">=</span> <span class="n">shift</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>        <span class="k">return</span> <span class="n">x</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SingleStreamBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>    <span class="p">):</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>            <span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>            <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>        <span class="p">)</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">QKNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_act</span> <span class="o">=</span> <span class="n">SiLUActivation</span><span class="p">()</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>        <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>        <span class="n">mod</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>        <span class="n">mod_shift</span><span class="p">,</span> <span class="n">mod_scale</span><span class="p">,</span> <span class="n">mod_gate</span> <span class="o">=</span> <span class="n">mod</span>  <span class="c1"># type: ignore</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>        <span class="n">x_mod</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">mod_scale</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">mod_shift</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>        <span class="n">qkv</span><span class="p">,</span> <span class="n">mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x_mod</span><span class="p">),</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>            <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">],</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>        <span class="p">)</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>        <span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>        <span class="c1"># compute activation in mlp stream, cat again and run second linear layer</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">attn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_act</span><span class="p">(</span><span class="n">mlp</span><span class="p">)),</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">mod_gate</span> <span class="o">*</span> <span class="n">output</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a><span class="k">class</span><span class="w"> </span><span class="nc">DoubleStreamBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>    <span class="p">):</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>        <span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>        <span class="k">assert</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">hidden_size</span><span class="si">=}</span><span class="s2"> must be divisible by </span><span class="si">{</span><span class="n">num_heads</span><span class="si">=}</span><span class="s2">&quot;</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>            <span class="n">dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>        <span class="p">)</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>            <span class="n">SiLUActivation</span><span class="p">(),</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a>        <span class="p">)</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>            <span class="n">dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a>        <span class="p">)</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a>                <span class="n">hidden_size</span><span class="p">,</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a>                <span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a>            <span class="p">),</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>            <span class="n">SiLUActivation</span><span class="p">(),</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>        <span class="p">)</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a>        <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a>        <span class="n">txt</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a>        <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>        <span class="n">pe_ctx</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>        <span class="n">mod_img</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>        <span class="n">mod_txt</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>        <span class="n">img_mod1</span><span class="p">,</span> <span class="n">img_mod2</span> <span class="o">=</span> <span class="n">mod_img</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a>        <span class="n">txt_mod1</span><span class="p">,</span> <span class="n">txt_mod2</span> <span class="o">=</span> <span class="n">mod_txt</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a>        <span class="n">img_mod1_shift</span><span class="p">,</span> <span class="n">img_mod1_scale</span><span class="p">,</span> <span class="n">img_mod1_gate</span> <span class="o">=</span> <span class="n">img_mod1</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a>        <span class="n">img_mod2_shift</span><span class="p">,</span> <span class="n">img_mod2_scale</span><span class="p">,</span> <span class="n">img_mod2_gate</span> <span class="o">=</span> <span class="n">img_mod2</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a>        <span class="n">txt_mod1_shift</span><span class="p">,</span> <span class="n">txt_mod1_scale</span><span class="p">,</span> <span class="n">txt_mod1_gate</span> <span class="o">=</span> <span class="n">txt_mod1</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a>        <span class="n">txt_mod2_shift</span><span class="p">,</span> <span class="n">txt_mod2_scale</span><span class="p">,</span> <span class="n">txt_mod2_gate</span> <span class="o">=</span> <span class="n">txt_mod2</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a>        <span class="c1"># prepare image for attention</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a>        <span class="n">img_modulated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_norm1</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a>        <span class="n">img_modulated</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">img_mod1_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">img_modulated</span> <span class="o">+</span> <span class="n">img_mod1_shift</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a>        <span class="n">img_qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">img_modulated</span><span class="p">)</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>        <span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span><span class="p">,</span> <span class="n">img_v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">img_qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>        <span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span><span class="p">,</span> <span class="n">img_v</span><span class="p">)</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a>        <span class="c1"># prepare txt for attention</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a>        <span class="n">txt_modulated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm1</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>        <span class="n">txt_modulated</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">txt_mod1_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">txt_modulated</span> <span class="o">+</span> <span class="n">txt_mod1_shift</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>        <span class="n">txt_qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">txt_modulated</span><span class="p">)</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>        <span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span><span class="p">,</span> <span class="n">txt_v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">txt_qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>        <span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span><span class="p">,</span> <span class="n">txt_v</span><span class="p">)</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_q</span><span class="p">,</span> <span class="n">img_q</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_k</span><span class="p">,</span> <span class="n">img_k</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_v</span><span class="p">,</span> <span class="n">img_v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pe_ctx</span><span class="p">,</span> <span class="n">pe</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a>        <span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a>        <span class="n">txt_attn</span><span class="p">,</span> <span class="n">img_attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">txt_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">attn</span><span class="p">[:,</span> <span class="n">txt_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">:]</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>        <span class="c1"># calculate the img blocks</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">+</span> <span class="n">img_mod1_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">img_attn</span><span class="p">)</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">+</span> <span class="n">img_mod2_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_mlp</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">img_mod2_scale</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_norm2</span><span class="p">(</span><span class="n">img</span><span class="p">))</span> <span class="o">+</span> <span class="n">img_mod2_shift</span><span class="p">)</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>        <span class="c1"># calculate the txt blocks</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span> <span class="o">+</span> <span class="n">txt_mod1_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">txt_attn</span><span class="p">)</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span> <span class="o">+</span> <span class="n">txt_mod2_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_mlp</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">txt_mod2_scale</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">txt_norm2</span><span class="p">(</span><span class="n">txt</span><span class="p">))</span> <span class="o">+</span> <span class="n">txt_mod2_shift</span><span class="p">)</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">txt</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a><span class="k">class</span><span class="w"> </span><span class="nc">MLPEmbedder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">silu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a><span class="k">class</span><span class="w"> </span><span class="nc">EmbedND</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">axes_dim</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span> <span class="o">=</span> <span class="n">axes_dim</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>        <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>            <span class="p">[</span><span class="n">rope</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">))],</span>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>            <span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>        <span class="p">)</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>        <span class="k">return</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a><span class="k">def</span><span class="w"> </span><span class="nf">timestep_embedding</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">max_period</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">time_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1000.0</span><span class="p">):</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a><span class="sd">    Create sinusoidal timestep embeddings.</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a><span class="sd">    :param t: a 1-D Tensor of N indices, one per batch element.</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a><span class="sd">                      These may be fractional.</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a><span class="sd">    :param dim: the dimension of the output.</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a><span class="sd">    :param max_period: controls the minimum frequency of the embeddings.</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a><span class="sd">    :return: an (N, D) Tensor of positional embeddings.</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>    <span class="n">t</span> <span class="o">=</span> <span class="n">time_factor</span> <span class="o">*</span> <span class="n">t</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>    <span class="n">half</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>    <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>        <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">max_period</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">half</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="n">half</span>  <span class="c1"># float32 originally</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>    <span class="p">)</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>    <span class="n">args</span> <span class="o">=</span> <span class="n">t</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="n">freqs</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>    <span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">args</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>    <span class="k">if</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>        <span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">embedding</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">embedding</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>    <span class="k">return</span> <span class="n">embedding</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a><span class="k">class</span><span class="w"> </span><span class="nc">RMSNorm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>        <span class="n">rrms</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">rrms</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x_dtype</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a><span class="k">class</span><span class="w"> </span><span class="nc">QKNorm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">query_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">key_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_norm</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_norm</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a>        <span class="k">return</span> <span class="n">q</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">k</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a><span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos">428</span></a>    <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">apply_rope</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos">429</span></a>
</span><span id="L-430"><a href="#L-430"><span class="linenos">430</span></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos">431</span></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;B H L D -&gt; B L (H D)&quot;</span><span class="p">)</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos">432</span></a>
</span><span id="L-433"><a href="#L-433"><span class="linenos">433</span></a>    <span class="k">return</span> <span class="n">x</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos">434</span></a>
</span><span id="L-435"><a href="#L-435"><span class="linenos">435</span></a>
</span><span id="L-436"><a href="#L-436"><span class="linenos">436</span></a><span class="k">def</span><span class="w"> </span><span class="nf">rope</span><span class="p">(</span><span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos">437</span></a>    <span class="k">assert</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="L-438"><a href="#L-438"><span class="linenos">438</span></a>    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos">439</span></a>    <span class="n">omega</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta</span><span class="o">**</span><span class="n">scale</span><span class="p">)</span>
</span><span id="L-440"><a href="#L-440"><span class="linenos">440</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...n,d-&gt;...nd&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos">441</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">out</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos">442</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s2">&quot;b n d (i j) -&gt; b n d i j&quot;</span><span class="p">,</span> <span class="n">i</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="L-443"><a href="#L-443"><span class="linenos">443</span></a>    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos">444</span></a>
</span><span id="L-445"><a href="#L-445"><span class="linenos">445</span></a>
</span><span id="L-446"><a href="#L-446"><span class="linenos">446</span></a><span class="k">def</span><span class="w"> </span><span class="nf">apply_rope</span><span class="p">(</span><span class="n">xq</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">xk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos">447</span></a>    <span class="n">xq_</span> <span class="o">=</span> <span class="n">xq</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xq</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos">448</span></a>    <span class="n">xk_</span> <span class="o">=</span> <span class="n">xk</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xk</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos">449</span></a>    <span class="n">xq_out</span> <span class="o">=</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">xq_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xq_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos">450</span></a>    <span class="n">xk_out</span> <span class="o">=</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">xk_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xk_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos">451</span></a>    <span class="k">return</span> <span class="n">xq_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xq</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xq</span><span class="p">),</span> <span class="n">xk_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xk</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
</span></pre></div>


            </section>
                <section id="Flux2Params">
                            <input id="Flux2Params-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
                    <div class="decorator decorator-dataclass">@dataclass</div>

    <span class="def">class</span>
    <span class="name">Flux2Params</span>:

                <label class="view-source-button" for="Flux2Params-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Flux2Params"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Flux2Params-15"><a href="#Flux2Params-15"><span class="linenos">15</span></a><span class="nd">@dataclass</span>
</span><span id="Flux2Params-16"><a href="#Flux2Params-16"><span class="linenos">16</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Flux2Params</span><span class="p">:</span>
</span><span id="Flux2Params-17"><a href="#Flux2Params-17"><span class="linenos">17</span></a>    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
</span><span id="Flux2Params-18"><a href="#Flux2Params-18"><span class="linenos">18</span></a>    <span class="n">context_in_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15360</span>
</span><span id="Flux2Params-19"><a href="#Flux2Params-19"><span class="linenos">19</span></a>    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6144</span>
</span><span id="Flux2Params-20"><a href="#Flux2Params-20"><span class="linenos">20</span></a>    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>
</span><span id="Flux2Params-21"><a href="#Flux2Params-21"><span class="linenos">21</span></a>    <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>
</span><span id="Flux2Params-22"><a href="#Flux2Params-22"><span class="linenos">22</span></a>    <span class="n">depth_single_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>
</span><span id="Flux2Params-23"><a href="#Flux2Params-23"><span class="linenos">23</span></a>    <span class="n">axes_dim</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
</span><span id="Flux2Params-24"><a href="#Flux2Params-24"><span class="linenos">24</span></a>    <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span>
</span><span id="Flux2Params-25"><a href="#Flux2Params-25"><span class="linenos">25</span></a>    <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span>
</span></pre></div>


    

                            <div id="Flux2Params.__init__" class="classattr">
                                <div class="attr function">
            
        <span class="name">Flux2Params</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>,</span><span class="param">	<span class="n">context_in_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15360</span>,</span><span class="param">	<span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6144</span>,</span><span class="param">	<span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>,</span><span class="param">	<span class="n">depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>,</span><span class="param">	<span class="n">depth_single_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>,</span><span class="param">	<span class="n">axes_dim</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">factory</span><span class="o">&gt;</span>,</span><span class="param">	<span class="n">theta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span>,</span><span class="param">	<span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span></span>)</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.__init__"></a>
    
    

                            </div>
                            <div id="Flux2Params.in_channels" class="classattr">
                                <div class="attr variable">
            <span class="name">in_channels</span><span class="annotation">: int</span>        =
<span class="default_value">128</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.in_channels"></a>
    
    

                            </div>
                            <div id="Flux2Params.context_in_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">context_in_dim</span><span class="annotation">: int</span>        =
<span class="default_value">15360</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.context_in_dim"></a>
    
    

                            </div>
                            <div id="Flux2Params.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span><span class="annotation">: int</span>        =
<span class="default_value">6144</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.hidden_size"></a>
    
    

                            </div>
                            <div id="Flux2Params.num_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_heads</span><span class="annotation">: int</span>        =
<span class="default_value">48</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.num_heads"></a>
    
    

                            </div>
                            <div id="Flux2Params.depth" class="classattr">
                                <div class="attr variable">
            <span class="name">depth</span><span class="annotation">: int</span>        =
<span class="default_value">8</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.depth"></a>
    
    

                            </div>
                            <div id="Flux2Params.depth_single_blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">depth_single_blocks</span><span class="annotation">: int</span>        =
<span class="default_value">48</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.depth_single_blocks"></a>
    
    

                            </div>
                            <div id="Flux2Params.axes_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">axes_dim</span><span class="annotation">: list[int]</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.axes_dim"></a>
    
    

                            </div>
                            <div id="Flux2Params.theta" class="classattr">
                                <div class="attr variable">
            <span class="name">theta</span><span class="annotation">: int</span>        =
<span class="default_value">2000</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.theta"></a>
    
    

                            </div>
                            <div id="Flux2Params.mlp_ratio" class="classattr">
                                <div class="attr variable">
            <span class="name">mlp_ratio</span><span class="annotation">: float</span>        =
<span class="default_value">3.0</span>

        
    </div>
    <a class="headerlink" href="#Flux2Params.mlp_ratio"></a>
    
    

                            </div>
                </section>
                <section id="Flux2">
                            <input id="Flux2-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Flux2</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="Flux2-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Flux2"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Flux2-28"><a href="#Flux2-28"><span class="linenos"> 28</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Flux2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="Flux2-29"><a href="#Flux2-29"><span class="linenos"> 29</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Flux2Params</span><span class="p">):</span>
</span><span id="Flux2-30"><a href="#Flux2-30"><span class="linenos"> 30</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="Flux2-31"><a href="#Flux2-31"><span class="linenos"> 31</span></a>
</span><span id="Flux2-32"><a href="#Flux2-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">in_channels</span>
</span><span id="Flux2-33"><a href="#Flux2-33"><span class="linenos"> 33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">in_channels</span>
</span><span id="Flux2-34"><a href="#Flux2-34"><span class="linenos"> 34</span></a>        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="Flux2-35"><a href="#Flux2-35"><span class="linenos"> 35</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Hidden size </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2"> must be divisible by num_heads </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="Flux2-36"><a href="#Flux2-36"><span class="linenos"> 36</span></a>        <span class="n">pe_dim</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
</span><span id="Flux2-37"><a href="#Flux2-37"><span class="linenos"> 37</span></a>        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pe_dim</span><span class="p">:</span>
</span><span id="Flux2-38"><a href="#Flux2-38"><span class="linenos"> 38</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="si">}</span><span class="s2"> but expected positional dim </span><span class="si">{</span><span class="n">pe_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="Flux2-39"><a href="#Flux2-39"><span class="linenos"> 39</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span>
</span><span id="Flux2-40"><a href="#Flux2-40"><span class="linenos"> 40</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
</span><span id="Flux2-41"><a href="#Flux2-41"><span class="linenos"> 41</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span> <span class="o">=</span> <span class="n">EmbedND</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">pe_dim</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">axes_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">)</span>
</span><span id="Flux2-42"><a href="#Flux2-42"><span class="linenos"> 42</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="Flux2-43"><a href="#Flux2-43"><span class="linenos"> 43</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">time_in</span> <span class="o">=</span> <span class="n">MLPEmbedder</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="Flux2-44"><a href="#Flux2-44"><span class="linenos"> 44</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">guidance_in</span> <span class="o">=</span> <span class="n">MLPEmbedder</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="Flux2-45"><a href="#Flux2-45"><span class="linenos"> 45</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">context_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="Flux2-46"><a href="#Flux2-46"><span class="linenos"> 46</span></a>
</span><span id="Flux2-47"><a href="#Flux2-47"><span class="linenos"> 47</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="Flux2-48"><a href="#Flux2-48"><span class="linenos"> 48</span></a>            <span class="p">[</span>
</span><span id="Flux2-49"><a href="#Flux2-49"><span class="linenos"> 49</span></a>                <span class="n">DoubleStreamBlock</span><span class="p">(</span>
</span><span id="Flux2-50"><a href="#Flux2-50"><span class="linenos"> 50</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2-51"><a href="#Flux2-51"><span class="linenos"> 51</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="Flux2-52"><a href="#Flux2-52"><span class="linenos"> 52</span></a>                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">,</span>
</span><span id="Flux2-53"><a href="#Flux2-53"><span class="linenos"> 53</span></a>                <span class="p">)</span>
</span><span id="Flux2-54"><a href="#Flux2-54"><span class="linenos"> 54</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">depth</span><span class="p">)</span>
</span><span id="Flux2-55"><a href="#Flux2-55"><span class="linenos"> 55</span></a>            <span class="p">]</span>
</span><span id="Flux2-56"><a href="#Flux2-56"><span class="linenos"> 56</span></a>        <span class="p">)</span>
</span><span id="Flux2-57"><a href="#Flux2-57"><span class="linenos"> 57</span></a>
</span><span id="Flux2-58"><a href="#Flux2-58"><span class="linenos"> 58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">single_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="Flux2-59"><a href="#Flux2-59"><span class="linenos"> 59</span></a>            <span class="p">[</span>
</span><span id="Flux2-60"><a href="#Flux2-60"><span class="linenos"> 60</span></a>                <span class="n">SingleStreamBlock</span><span class="p">(</span>
</span><span id="Flux2-61"><a href="#Flux2-61"><span class="linenos"> 61</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2-62"><a href="#Flux2-62"><span class="linenos"> 62</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="Flux2-63"><a href="#Flux2-63"><span class="linenos"> 63</span></a>                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">,</span>
</span><span id="Flux2-64"><a href="#Flux2-64"><span class="linenos"> 64</span></a>                <span class="p">)</span>
</span><span id="Flux2-65"><a href="#Flux2-65"><span class="linenos"> 65</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">depth_single_blocks</span><span class="p">)</span>
</span><span id="Flux2-66"><a href="#Flux2-66"><span class="linenos"> 66</span></a>            <span class="p">]</span>
</span><span id="Flux2-67"><a href="#Flux2-67"><span class="linenos"> 67</span></a>        <span class="p">)</span>
</span><span id="Flux2-68"><a href="#Flux2-68"><span class="linenos"> 68</span></a>
</span><span id="Flux2-69"><a href="#Flux2-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_img</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span>
</span><span id="Flux2-70"><a href="#Flux2-70"><span class="linenos"> 70</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2-71"><a href="#Flux2-71"><span class="linenos"> 71</span></a>            <span class="n">double</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2-72"><a href="#Flux2-72"><span class="linenos"> 72</span></a>            <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2-73"><a href="#Flux2-73"><span class="linenos"> 73</span></a>        <span class="p">)</span>
</span><span id="Flux2-74"><a href="#Flux2-74"><span class="linenos"> 74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_txt</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span>
</span><span id="Flux2-75"><a href="#Flux2-75"><span class="linenos"> 75</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2-76"><a href="#Flux2-76"><span class="linenos"> 76</span></a>            <span class="n">double</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2-77"><a href="#Flux2-77"><span class="linenos"> 77</span></a>            <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2-78"><a href="#Flux2-78"><span class="linenos"> 78</span></a>        <span class="p">)</span>
</span><span id="Flux2-79"><a href="#Flux2-79"><span class="linenos"> 79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">single_stream_modulation</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">double</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="Flux2-80"><a href="#Flux2-80"><span class="linenos"> 80</span></a>
</span><span id="Flux2-81"><a href="#Flux2-81"><span class="linenos"> 81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">LastLayer</span><span class="p">(</span>
</span><span id="Flux2-82"><a href="#Flux2-82"><span class="linenos"> 82</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2-83"><a href="#Flux2-83"><span class="linenos"> 83</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
</span><span id="Flux2-84"><a href="#Flux2-84"><span class="linenos"> 84</span></a>        <span class="p">)</span>
</span><span id="Flux2-85"><a href="#Flux2-85"><span class="linenos"> 85</span></a>
</span><span id="Flux2-86"><a href="#Flux2-86"><span class="linenos"> 86</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="Flux2-87"><a href="#Flux2-87"><span class="linenos"> 87</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Flux2-88"><a href="#Flux2-88"><span class="linenos"> 88</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2-89"><a href="#Flux2-89"><span class="linenos"> 89</span></a>        <span class="n">x_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2-90"><a href="#Flux2-90"><span class="linenos"> 90</span></a>        <span class="n">timesteps</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2-91"><a href="#Flux2-91"><span class="linenos"> 91</span></a>        <span class="n">ctx</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2-92"><a href="#Flux2-92"><span class="linenos"> 92</span></a>        <span class="n">ctx_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2-93"><a href="#Flux2-93"><span class="linenos"> 93</span></a>        <span class="n">guidance</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2-94"><a href="#Flux2-94"><span class="linenos"> 94</span></a>        <span class="n">layer_dropouts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Flux2-95"><a href="#Flux2-95"><span class="linenos"> 95</span></a>    <span class="p">):</span>
</span><span id="Flux2-96"><a href="#Flux2-96"><span class="linenos"> 96</span></a>        <span class="n">num_txt_tokens</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="Flux2-97"><a href="#Flux2-97"><span class="linenos"> 97</span></a>
</span><span id="Flux2-98"><a href="#Flux2-98"><span class="linenos"> 98</span></a>        <span class="n">timestep_emb</span> <span class="o">=</span> <span class="n">timestep_embedding</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</span><span id="Flux2-99"><a href="#Flux2-99"><span class="linenos"> 99</span></a>        <span class="n">vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_in</span><span class="p">(</span><span class="n">timestep_emb</span><span class="p">)</span>
</span><span id="Flux2-100"><a href="#Flux2-100"><span class="linenos">100</span></a>        <span class="n">guidance_emb</span> <span class="o">=</span> <span class="n">timestep_embedding</span><span class="p">(</span><span class="n">guidance</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</span><span id="Flux2-101"><a href="#Flux2-101"><span class="linenos">101</span></a>        <span class="n">vec</span> <span class="o">=</span> <span class="n">vec</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">guidance_in</span><span class="p">(</span><span class="n">guidance_emb</span><span class="p">)</span>
</span><span id="Flux2-102"><a href="#Flux2-102"><span class="linenos">102</span></a>
</span><span id="Flux2-103"><a href="#Flux2-103"><span class="linenos">103</span></a>        <span class="n">double_block_mod_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_img</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2-104"><a href="#Flux2-104"><span class="linenos">104</span></a>        <span class="n">double_block_mod_txt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_txt</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2-105"><a href="#Flux2-105"><span class="linenos">105</span></a>        <span class="n">single_block_mod</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_stream_modulation</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2-106"><a href="#Flux2-106"><span class="linenos">106</span></a>
</span><span id="Flux2-107"><a href="#Flux2-107"><span class="linenos">107</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_in</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="Flux2-108"><a href="#Flux2-108"><span class="linenos">108</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_in</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
</span><span id="Flux2-109"><a href="#Flux2-109"><span class="linenos">109</span></a>
</span><span id="Flux2-110"><a href="#Flux2-110"><span class="linenos">110</span></a>        <span class="n">pe_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span><span class="p">(</span><span class="n">x_ids</span><span class="p">)</span>
</span><span id="Flux2-111"><a href="#Flux2-111"><span class="linenos">111</span></a>        <span class="n">pe_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span><span class="p">(</span><span class="n">ctx_ids</span><span class="p">)</span>
</span><span id="Flux2-112"><a href="#Flux2-112"><span class="linenos">112</span></a>
</span><span id="Flux2-113"><a href="#Flux2-113"><span class="linenos">113</span></a>        <span class="n">img</span><span class="p">,</span> <span class="n">txt</span> <span class="o">=</span> <span class="n">process_blocks_with_dropout</span><span class="p">(</span>
</span><span id="Flux2-114"><a href="#Flux2-114"><span class="linenos">114</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span><span class="p">,</span>
</span><span id="Flux2-115"><a href="#Flux2-115"><span class="linenos">115</span></a>            <span class="n">layer_dropouts</span><span class="p">,</span>  <span class="c1"># Would need to be added to flux2 forward signature</span>
</span><span id="Flux2-116"><a href="#Flux2-116"><span class="linenos">116</span></a>            <span class="mi">0</span><span class="p">,</span>
</span><span id="Flux2-117"><a href="#Flux2-117"><span class="linenos">117</span></a>            <span class="s2">&quot;double&quot;</span><span class="p">,</span>
</span><span id="Flux2-118"><a href="#Flux2-118"><span class="linenos">118</span></a>            <span class="k">lambda</span> <span class="n">block</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pe_x</span><span class="p">,</span> <span class="n">pe_ctx</span><span class="p">,</span> <span class="n">double_block_mod_img</span><span class="p">,</span> <span class="n">double_block_mod_txt</span><span class="p">),</span>
</span><span id="Flux2-119"><a href="#Flux2-119"><span class="linenos">119</span></a>            <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">txt</span><span class="p">),</span>
</span><span id="Flux2-120"><a href="#Flux2-120"><span class="linenos">120</span></a>        <span class="p">)</span>
</span><span id="Flux2-121"><a href="#Flux2-121"><span class="linenos">121</span></a>
</span><span id="Flux2-122"><a href="#Flux2-122"><span class="linenos">122</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt</span><span class="p">,</span> <span class="n">img</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="Flux2-123"><a href="#Flux2-123"><span class="linenos">123</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pe_ctx</span><span class="p">,</span> <span class="n">pe_x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="Flux2-124"><a href="#Flux2-124"><span class="linenos">124</span></a>
</span><span id="Flux2-125"><a href="#Flux2-125"><span class="linenos">125</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">process_blocks_with_dropout</span><span class="p">(</span>
</span><span id="Flux2-126"><a href="#Flux2-126"><span class="linenos">126</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">single_blocks</span><span class="p">,</span>
</span><span id="Flux2-127"><a href="#Flux2-127"><span class="linenos">127</span></a>            <span class="n">layer_dropouts</span><span class="p">,</span>  <span class="c1"># Would need to be added to flux2 forward signature</span>
</span><span id="Flux2-128"><a href="#Flux2-128"><span class="linenos">128</span></a>            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span><span class="p">),</span>
</span><span id="Flux2-129"><a href="#Flux2-129"><span class="linenos">129</span></a>            <span class="s2">&quot;single&quot;</span><span class="p">,</span>
</span><span id="Flux2-130"><a href="#Flux2-130"><span class="linenos">130</span></a>            <span class="k">lambda</span> <span class="n">block</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="n">single_block_mod</span><span class="p">),</span>
</span><span id="Flux2-131"><a href="#Flux2-131"><span class="linenos">131</span></a>            <span class="n">img</span><span class="p">,</span>
</span><span id="Flux2-132"><a href="#Flux2-132"><span class="linenos">132</span></a>        <span class="p">)</span>
</span><span id="Flux2-133"><a href="#Flux2-133"><span class="linenos">133</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="n">num_txt_tokens</span><span class="p">:,</span> <span class="o">...</span><span class="p">]</span>
</span><span id="Flux2-134"><a href="#Flux2-134"><span class="linenos">134</span></a>
</span><span id="Flux2-135"><a href="#Flux2-135"><span class="linenos">135</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2-136"><a href="#Flux2-136"><span class="linenos">136</span></a>        <span class="k">return</span> <span class="n">img</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#Flux2.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#Flux2.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="Flux2.__init__" class="classattr">
                                        <input id="Flux2.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Flux2</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">params</span><span class="p">:</span> <span class="n"><a href="#Flux2Params">Flux2Params</a></span></span>)</span>

                <label class="view-source-button" for="Flux2.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Flux2.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Flux2.__init__-29"><a href="#Flux2.__init__-29"><span class="linenos">29</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Flux2Params</span><span class="p">):</span>
</span><span id="Flux2.__init__-30"><a href="#Flux2.__init__-30"><span class="linenos">30</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="Flux2.__init__-31"><a href="#Flux2.__init__-31"><span class="linenos">31</span></a>
</span><span id="Flux2.__init__-32"><a href="#Flux2.__init__-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">in_channels</span>
</span><span id="Flux2.__init__-33"><a href="#Flux2.__init__-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">in_channels</span>
</span><span id="Flux2.__init__-34"><a href="#Flux2.__init__-34"><span class="linenos">34</span></a>        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">%</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="Flux2.__init__-35"><a href="#Flux2.__init__-35"><span class="linenos">35</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Hidden size </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2"> must be divisible by num_heads </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="Flux2.__init__-36"><a href="#Flux2.__init__-36"><span class="linenos">36</span></a>        <span class="n">pe_dim</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
</span><span id="Flux2.__init__-37"><a href="#Flux2.__init__-37"><span class="linenos">37</span></a>        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pe_dim</span><span class="p">:</span>
</span><span id="Flux2.__init__-38"><a href="#Flux2.__init__-38"><span class="linenos">38</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="si">}</span><span class="s2"> but expected positional dim </span><span class="si">{</span><span class="n">pe_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="Flux2.__init__-39"><a href="#Flux2.__init__-39"><span class="linenos">39</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">hidden_size</span>
</span><span id="Flux2.__init__-40"><a href="#Flux2.__init__-40"><span class="linenos">40</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">num_heads</span>
</span><span id="Flux2.__init__-41"><a href="#Flux2.__init__-41"><span class="linenos">41</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span> <span class="o">=</span> <span class="n">EmbedND</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">pe_dim</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">axes_dim</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">)</span>
</span><span id="Flux2.__init__-42"><a href="#Flux2.__init__-42"><span class="linenos">42</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="Flux2.__init__-43"><a href="#Flux2.__init__-43"><span class="linenos">43</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">time_in</span> <span class="o">=</span> <span class="n">MLPEmbedder</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="Flux2.__init__-44"><a href="#Flux2.__init__-44"><span class="linenos">44</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">guidance_in</span> <span class="o">=</span> <span class="n">MLPEmbedder</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="Flux2.__init__-45"><a href="#Flux2.__init__-45"><span class="linenos">45</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">context_in_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="Flux2.__init__-46"><a href="#Flux2.__init__-46"><span class="linenos">46</span></a>
</span><span id="Flux2.__init__-47"><a href="#Flux2.__init__-47"><span class="linenos">47</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="Flux2.__init__-48"><a href="#Flux2.__init__-48"><span class="linenos">48</span></a>            <span class="p">[</span>
</span><span id="Flux2.__init__-49"><a href="#Flux2.__init__-49"><span class="linenos">49</span></a>                <span class="n">DoubleStreamBlock</span><span class="p">(</span>
</span><span id="Flux2.__init__-50"><a href="#Flux2.__init__-50"><span class="linenos">50</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2.__init__-51"><a href="#Flux2.__init__-51"><span class="linenos">51</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="Flux2.__init__-52"><a href="#Flux2.__init__-52"><span class="linenos">52</span></a>                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">,</span>
</span><span id="Flux2.__init__-53"><a href="#Flux2.__init__-53"><span class="linenos">53</span></a>                <span class="p">)</span>
</span><span id="Flux2.__init__-54"><a href="#Flux2.__init__-54"><span class="linenos">54</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">depth</span><span class="p">)</span>
</span><span id="Flux2.__init__-55"><a href="#Flux2.__init__-55"><span class="linenos">55</span></a>            <span class="p">]</span>
</span><span id="Flux2.__init__-56"><a href="#Flux2.__init__-56"><span class="linenos">56</span></a>        <span class="p">)</span>
</span><span id="Flux2.__init__-57"><a href="#Flux2.__init__-57"><span class="linenos">57</span></a>
</span><span id="Flux2.__init__-58"><a href="#Flux2.__init__-58"><span class="linenos">58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">single_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span><span id="Flux2.__init__-59"><a href="#Flux2.__init__-59"><span class="linenos">59</span></a>            <span class="p">[</span>
</span><span id="Flux2.__init__-60"><a href="#Flux2.__init__-60"><span class="linenos">60</span></a>                <span class="n">SingleStreamBlock</span><span class="p">(</span>
</span><span id="Flux2.__init__-61"><a href="#Flux2.__init__-61"><span class="linenos">61</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2.__init__-62"><a href="#Flux2.__init__-62"><span class="linenos">62</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="Flux2.__init__-63"><a href="#Flux2.__init__-63"><span class="linenos">63</span></a>                    <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">,</span>
</span><span id="Flux2.__init__-64"><a href="#Flux2.__init__-64"><span class="linenos">64</span></a>                <span class="p">)</span>
</span><span id="Flux2.__init__-65"><a href="#Flux2.__init__-65"><span class="linenos">65</span></a>                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">depth_single_blocks</span><span class="p">)</span>
</span><span id="Flux2.__init__-66"><a href="#Flux2.__init__-66"><span class="linenos">66</span></a>            <span class="p">]</span>
</span><span id="Flux2.__init__-67"><a href="#Flux2.__init__-67"><span class="linenos">67</span></a>        <span class="p">)</span>
</span><span id="Flux2.__init__-68"><a href="#Flux2.__init__-68"><span class="linenos">68</span></a>
</span><span id="Flux2.__init__-69"><a href="#Flux2.__init__-69"><span class="linenos">69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_img</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span>
</span><span id="Flux2.__init__-70"><a href="#Flux2.__init__-70"><span class="linenos">70</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2.__init__-71"><a href="#Flux2.__init__-71"><span class="linenos">71</span></a>            <span class="n">double</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2.__init__-72"><a href="#Flux2.__init__-72"><span class="linenos">72</span></a>            <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2.__init__-73"><a href="#Flux2.__init__-73"><span class="linenos">73</span></a>        <span class="p">)</span>
</span><span id="Flux2.__init__-74"><a href="#Flux2.__init__-74"><span class="linenos">74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_txt</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span>
</span><span id="Flux2.__init__-75"><a href="#Flux2.__init__-75"><span class="linenos">75</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2.__init__-76"><a href="#Flux2.__init__-76"><span class="linenos">76</span></a>            <span class="n">double</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2.__init__-77"><a href="#Flux2.__init__-77"><span class="linenos">77</span></a>            <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="Flux2.__init__-78"><a href="#Flux2.__init__-78"><span class="linenos">78</span></a>        <span class="p">)</span>
</span><span id="Flux2.__init__-79"><a href="#Flux2.__init__-79"><span class="linenos">79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">single_stream_modulation</span> <span class="o">=</span> <span class="n">Modulation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">double</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">disable_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="Flux2.__init__-80"><a href="#Flux2.__init__-80"><span class="linenos">80</span></a>
</span><span id="Flux2.__init__-81"><a href="#Flux2.__init__-81"><span class="linenos">81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">LastLayer</span><span class="p">(</span>
</span><span id="Flux2.__init__-82"><a href="#Flux2.__init__-82"><span class="linenos">82</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="Flux2.__init__-83"><a href="#Flux2.__init__-83"><span class="linenos">83</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
</span><span id="Flux2.__init__-84"><a href="#Flux2.__init__-84"><span class="linenos">84</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="Flux2.in_channels" class="classattr">
                                <div class="attr variable">
            <span class="name">in_channels</span>

        
    </div>
    <a class="headerlink" href="#Flux2.in_channels"></a>
    
    

                            </div>
                            <div id="Flux2.out_channels" class="classattr">
                                <div class="attr variable">
            <span class="name">out_channels</span>

        
    </div>
    <a class="headerlink" href="#Flux2.out_channels"></a>
    
    

                            </div>
                            <div id="Flux2.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#Flux2.hidden_size"></a>
    
    

                            </div>
                            <div id="Flux2.num_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_heads</span>

        
    </div>
    <a class="headerlink" href="#Flux2.num_heads"></a>
    
    

                            </div>
                            <div id="Flux2.pe_embedder" class="classattr">
                                <div class="attr variable">
            <span class="name">pe_embedder</span>

        
    </div>
    <a class="headerlink" href="#Flux2.pe_embedder"></a>
    
    

                            </div>
                            <div id="Flux2.img_in" class="classattr">
                                <div class="attr variable">
            <span class="name">img_in</span>

        
    </div>
    <a class="headerlink" href="#Flux2.img_in"></a>
    
    

                            </div>
                            <div id="Flux2.time_in" class="classattr">
                                <div class="attr variable">
            <span class="name">time_in</span>

        
    </div>
    <a class="headerlink" href="#Flux2.time_in"></a>
    
    

                            </div>
                            <div id="Flux2.guidance_in" class="classattr">
                                <div class="attr variable">
            <span class="name">guidance_in</span>

        
    </div>
    <a class="headerlink" href="#Flux2.guidance_in"></a>
    
    

                            </div>
                            <div id="Flux2.txt_in" class="classattr">
                                <div class="attr variable">
            <span class="name">txt_in</span>

        
    </div>
    <a class="headerlink" href="#Flux2.txt_in"></a>
    
    

                            </div>
                            <div id="Flux2.double_blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">double_blocks</span>

        
    </div>
    <a class="headerlink" href="#Flux2.double_blocks"></a>
    
    

                            </div>
                            <div id="Flux2.single_blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">single_blocks</span>

        
    </div>
    <a class="headerlink" href="#Flux2.single_blocks"></a>
    
    

                            </div>
                            <div id="Flux2.double_stream_modulation_img" class="classattr">
                                <div class="attr variable">
            <span class="name">double_stream_modulation_img</span>

        
    </div>
    <a class="headerlink" href="#Flux2.double_stream_modulation_img"></a>
    
    

                            </div>
                            <div id="Flux2.double_stream_modulation_txt" class="classattr">
                                <div class="attr variable">
            <span class="name">double_stream_modulation_txt</span>

        
    </div>
    <a class="headerlink" href="#Flux2.double_stream_modulation_txt"></a>
    
    

                            </div>
                            <div id="Flux2.single_stream_modulation" class="classattr">
                                <div class="attr variable">
            <span class="name">single_stream_modulation</span>

        
    </div>
    <a class="headerlink" href="#Flux2.single_stream_modulation"></a>
    
    

                            </div>
                            <div id="Flux2.final_layer" class="classattr">
                                <div class="attr variable">
            <span class="name">final_layer</span>

        
    </div>
    <a class="headerlink" href="#Flux2.final_layer"></a>
    
    

                            </div>
                            <div id="Flux2.forward" class="classattr">
                                        <input id="Flux2.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">x_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">timesteps</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">ctx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">ctx_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">guidance</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">layer_dropouts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Flux2.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Flux2.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Flux2.forward-86"><a href="#Flux2.forward-86"><span class="linenos"> 86</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="Flux2.forward-87"><a href="#Flux2.forward-87"><span class="linenos"> 87</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Flux2.forward-88"><a href="#Flux2.forward-88"><span class="linenos"> 88</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2.forward-89"><a href="#Flux2.forward-89"><span class="linenos"> 89</span></a>        <span class="n">x_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2.forward-90"><a href="#Flux2.forward-90"><span class="linenos"> 90</span></a>        <span class="n">timesteps</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2.forward-91"><a href="#Flux2.forward-91"><span class="linenos"> 91</span></a>        <span class="n">ctx</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2.forward-92"><a href="#Flux2.forward-92"><span class="linenos"> 92</span></a>        <span class="n">ctx_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2.forward-93"><a href="#Flux2.forward-93"><span class="linenos"> 93</span></a>        <span class="n">guidance</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="Flux2.forward-94"><a href="#Flux2.forward-94"><span class="linenos"> 94</span></a>        <span class="n">layer_dropouts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Flux2.forward-95"><a href="#Flux2.forward-95"><span class="linenos"> 95</span></a>    <span class="p">):</span>
</span><span id="Flux2.forward-96"><a href="#Flux2.forward-96"><span class="linenos"> 96</span></a>        <span class="n">num_txt_tokens</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="Flux2.forward-97"><a href="#Flux2.forward-97"><span class="linenos"> 97</span></a>
</span><span id="Flux2.forward-98"><a href="#Flux2.forward-98"><span class="linenos"> 98</span></a>        <span class="n">timestep_emb</span> <span class="o">=</span> <span class="n">timestep_embedding</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</span><span id="Flux2.forward-99"><a href="#Flux2.forward-99"><span class="linenos"> 99</span></a>        <span class="n">vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_in</span><span class="p">(</span><span class="n">timestep_emb</span><span class="p">)</span>
</span><span id="Flux2.forward-100"><a href="#Flux2.forward-100"><span class="linenos">100</span></a>        <span class="n">guidance_emb</span> <span class="o">=</span> <span class="n">timestep_embedding</span><span class="p">(</span><span class="n">guidance</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</span><span id="Flux2.forward-101"><a href="#Flux2.forward-101"><span class="linenos">101</span></a>        <span class="n">vec</span> <span class="o">=</span> <span class="n">vec</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">guidance_in</span><span class="p">(</span><span class="n">guidance_emb</span><span class="p">)</span>
</span><span id="Flux2.forward-102"><a href="#Flux2.forward-102"><span class="linenos">102</span></a>
</span><span id="Flux2.forward-103"><a href="#Flux2.forward-103"><span class="linenos">103</span></a>        <span class="n">double_block_mod_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_img</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2.forward-104"><a href="#Flux2.forward-104"><span class="linenos">104</span></a>        <span class="n">double_block_mod_txt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">double_stream_modulation_txt</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2.forward-105"><a href="#Flux2.forward-105"><span class="linenos">105</span></a>        <span class="n">single_block_mod</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_stream_modulation</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2.forward-106"><a href="#Flux2.forward-106"><span class="linenos">106</span></a>
</span><span id="Flux2.forward-107"><a href="#Flux2.forward-107"><span class="linenos">107</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_in</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="Flux2.forward-108"><a href="#Flux2.forward-108"><span class="linenos">108</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_in</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
</span><span id="Flux2.forward-109"><a href="#Flux2.forward-109"><span class="linenos">109</span></a>
</span><span id="Flux2.forward-110"><a href="#Flux2.forward-110"><span class="linenos">110</span></a>        <span class="n">pe_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span><span class="p">(</span><span class="n">x_ids</span><span class="p">)</span>
</span><span id="Flux2.forward-111"><a href="#Flux2.forward-111"><span class="linenos">111</span></a>        <span class="n">pe_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_embedder</span><span class="p">(</span><span class="n">ctx_ids</span><span class="p">)</span>
</span><span id="Flux2.forward-112"><a href="#Flux2.forward-112"><span class="linenos">112</span></a>
</span><span id="Flux2.forward-113"><a href="#Flux2.forward-113"><span class="linenos">113</span></a>        <span class="n">img</span><span class="p">,</span> <span class="n">txt</span> <span class="o">=</span> <span class="n">process_blocks_with_dropout</span><span class="p">(</span>
</span><span id="Flux2.forward-114"><a href="#Flux2.forward-114"><span class="linenos">114</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span><span class="p">,</span>
</span><span id="Flux2.forward-115"><a href="#Flux2.forward-115"><span class="linenos">115</span></a>            <span class="n">layer_dropouts</span><span class="p">,</span>  <span class="c1"># Would need to be added to flux2 forward signature</span>
</span><span id="Flux2.forward-116"><a href="#Flux2.forward-116"><span class="linenos">116</span></a>            <span class="mi">0</span><span class="p">,</span>
</span><span id="Flux2.forward-117"><a href="#Flux2.forward-117"><span class="linenos">117</span></a>            <span class="s2">&quot;double&quot;</span><span class="p">,</span>
</span><span id="Flux2.forward-118"><a href="#Flux2.forward-118"><span class="linenos">118</span></a>            <span class="k">lambda</span> <span class="n">block</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pe_x</span><span class="p">,</span> <span class="n">pe_ctx</span><span class="p">,</span> <span class="n">double_block_mod_img</span><span class="p">,</span> <span class="n">double_block_mod_txt</span><span class="p">),</span>
</span><span id="Flux2.forward-119"><a href="#Flux2.forward-119"><span class="linenos">119</span></a>            <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">txt</span><span class="p">),</span>
</span><span id="Flux2.forward-120"><a href="#Flux2.forward-120"><span class="linenos">120</span></a>        <span class="p">)</span>
</span><span id="Flux2.forward-121"><a href="#Flux2.forward-121"><span class="linenos">121</span></a>
</span><span id="Flux2.forward-122"><a href="#Flux2.forward-122"><span class="linenos">122</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt</span><span class="p">,</span> <span class="n">img</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="Flux2.forward-123"><a href="#Flux2.forward-123"><span class="linenos">123</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pe_ctx</span><span class="p">,</span> <span class="n">pe_x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="Flux2.forward-124"><a href="#Flux2.forward-124"><span class="linenos">124</span></a>
</span><span id="Flux2.forward-125"><a href="#Flux2.forward-125"><span class="linenos">125</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">process_blocks_with_dropout</span><span class="p">(</span>
</span><span id="Flux2.forward-126"><a href="#Flux2.forward-126"><span class="linenos">126</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">single_blocks</span><span class="p">,</span>
</span><span id="Flux2.forward-127"><a href="#Flux2.forward-127"><span class="linenos">127</span></a>            <span class="n">layer_dropouts</span><span class="p">,</span>  <span class="c1"># Would need to be added to flux2 forward signature</span>
</span><span id="Flux2.forward-128"><a href="#Flux2.forward-128"><span class="linenos">128</span></a>            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">double_blocks</span><span class="p">),</span>
</span><span id="Flux2.forward-129"><a href="#Flux2.forward-129"><span class="linenos">129</span></a>            <span class="s2">&quot;single&quot;</span><span class="p">,</span>
</span><span id="Flux2.forward-130"><a href="#Flux2.forward-130"><span class="linenos">130</span></a>            <span class="k">lambda</span> <span class="n">block</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">block</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">pe</span><span class="p">,</span> <span class="n">single_block_mod</span><span class="p">),</span>
</span><span id="Flux2.forward-131"><a href="#Flux2.forward-131"><span class="linenos">131</span></a>            <span class="n">img</span><span class="p">,</span>
</span><span id="Flux2.forward-132"><a href="#Flux2.forward-132"><span class="linenos">132</span></a>        <span class="p">)</span>
</span><span id="Flux2.forward-133"><a href="#Flux2.forward-133"><span class="linenos">133</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="n">num_txt_tokens</span><span class="p">:,</span> <span class="o">...</span><span class="p">]</span>
</span><span id="Flux2.forward-134"><a href="#Flux2.forward-134"><span class="linenos">134</span></a>
</span><span id="Flux2.forward-135"><a href="#Flux2.forward-135"><span class="linenos">135</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
</span><span id="Flux2.forward-136"><a href="#Flux2.forward-136"><span class="linenos">136</span></a>        <span class="k">return</span> <span class="n">img</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="SelfAttention">
                            <input id="SelfAttention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">SelfAttention</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="SelfAttention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SelfAttention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SelfAttention-139"><a href="#SelfAttention-139"><span class="linenos">139</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="SelfAttention-140"><a href="#SelfAttention-140"><span class="linenos">140</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="SelfAttention-141"><a href="#SelfAttention-141"><span class="linenos">141</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SelfAttention-142"><a href="#SelfAttention-142"><span class="linenos">142</span></a>        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="SelfAttention-143"><a href="#SelfAttention-143"><span class="linenos">143</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="SelfAttention-144"><a href="#SelfAttention-144"><span class="linenos">144</span></a>    <span class="p">):</span>
</span><span id="SelfAttention-145"><a href="#SelfAttention-145"><span class="linenos">145</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="SelfAttention-146"><a href="#SelfAttention-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="SelfAttention-147"><a href="#SelfAttention-147"><span class="linenos">147</span></a>        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span><span id="SelfAttention-148"><a href="#SelfAttention-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="SelfAttention-149"><a href="#SelfAttention-149"><span class="linenos">149</span></a>
</span><span id="SelfAttention-150"><a href="#SelfAttention-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">QKNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
</span><span id="SelfAttention-151"><a href="#SelfAttention-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#SelfAttention.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#SelfAttention.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="SelfAttention.__init__" class="classattr">
                                        <input id="SelfAttention.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">SelfAttention</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span></span>)</span>

                <label class="view-source-button" for="SelfAttention.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SelfAttention.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SelfAttention.__init__-140"><a href="#SelfAttention.__init__-140"><span class="linenos">140</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="SelfAttention.__init__-141"><a href="#SelfAttention.__init__-141"><span class="linenos">141</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SelfAttention.__init__-142"><a href="#SelfAttention.__init__-142"><span class="linenos">142</span></a>        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="SelfAttention.__init__-143"><a href="#SelfAttention.__init__-143"><span class="linenos">143</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
</span><span id="SelfAttention.__init__-144"><a href="#SelfAttention.__init__-144"><span class="linenos">144</span></a>    <span class="p">):</span>
</span><span id="SelfAttention.__init__-145"><a href="#SelfAttention.__init__-145"><span class="linenos">145</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="SelfAttention.__init__-146"><a href="#SelfAttention.__init__-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="SelfAttention.__init__-147"><a href="#SelfAttention.__init__-147"><span class="linenos">147</span></a>        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span><span id="SelfAttention.__init__-148"><a href="#SelfAttention.__init__-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="SelfAttention.__init__-149"><a href="#SelfAttention.__init__-149"><span class="linenos">149</span></a>
</span><span id="SelfAttention.__init__-150"><a href="#SelfAttention.__init__-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">QKNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
</span><span id="SelfAttention.__init__-151"><a href="#SelfAttention.__init__-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="SelfAttention.num_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_heads</span>

        
    </div>
    <a class="headerlink" href="#SelfAttention.num_heads"></a>
    
    

                            </div>
                            <div id="SelfAttention.qkv" class="classattr">
                                <div class="attr variable">
            <span class="name">qkv</span>

        
    </div>
    <a class="headerlink" href="#SelfAttention.qkv"></a>
    
    

                            </div>
                            <div id="SelfAttention.norm" class="classattr">
                                <div class="attr variable">
            <span class="name">norm</span>

        
    </div>
    <a class="headerlink" href="#SelfAttention.norm"></a>
    
    

                            </div>
                            <div id="SelfAttention.proj" class="classattr">
                                <div class="attr variable">
            <span class="name">proj</span>

        
    </div>
    <a class="headerlink" href="#SelfAttention.proj"></a>
    
    

                            </div>
                </section>
                <section id="SiLUActivation">
                            <input id="SiLUActivation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">SiLUActivation</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="SiLUActivation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SiLUActivation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SiLUActivation-154"><a href="#SiLUActivation-154"><span class="linenos">154</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SiLUActivation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="SiLUActivation-155"><a href="#SiLUActivation-155"><span class="linenos">155</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="SiLUActivation-156"><a href="#SiLUActivation-156"><span class="linenos">156</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="SiLUActivation-157"><a href="#SiLUActivation-157"><span class="linenos">157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
</span><span id="SiLUActivation-158"><a href="#SiLUActivation-158"><span class="linenos">158</span></a>
</span><span id="SiLUActivation-159"><a href="#SiLUActivation-159"><span class="linenos">159</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="SiLUActivation-160"><a href="#SiLUActivation-160"><span class="linenos">160</span></a>        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="SiLUActivation-161"><a href="#SiLUActivation-161"><span class="linenos">161</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x2</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#SiLUActivation.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#SiLUActivation.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="SiLUActivation.__init__" class="classattr">
                                        <input id="SiLUActivation.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">SiLUActivation</span><span class="signature pdoc-code condensed">()</span>

                <label class="view-source-button" for="SiLUActivation.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SiLUActivation.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SiLUActivation.__init__-155"><a href="#SiLUActivation.__init__-155"><span class="linenos">155</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="SiLUActivation.__init__-156"><a href="#SiLUActivation.__init__-156"><span class="linenos">156</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="SiLUActivation.__init__-157"><a href="#SiLUActivation.__init__-157"><span class="linenos">157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="SiLUActivation.gate_fn" class="classattr">
                                <div class="attr variable">
            <span class="name">gate_fn</span>

        
    </div>
    <a class="headerlink" href="#SiLUActivation.gate_fn"></a>
    
    

                            </div>
                            <div id="SiLUActivation.forward" class="classattr">
                                        <input id="SiLUActivation.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="SiLUActivation.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SiLUActivation.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SiLUActivation.forward-159"><a href="#SiLUActivation.forward-159"><span class="linenos">159</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="SiLUActivation.forward-160"><a href="#SiLUActivation.forward-160"><span class="linenos">160</span></a>        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="SiLUActivation.forward-161"><a href="#SiLUActivation.forward-161"><span class="linenos">161</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_fn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x2</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="Modulation">
                            <input id="Modulation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Modulation</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="Modulation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Modulation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Modulation-164"><a href="#Modulation-164"><span class="linenos">164</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Modulation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="Modulation-165"><a href="#Modulation-165"><span class="linenos">165</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">double</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Modulation-166"><a href="#Modulation-166"><span class="linenos">166</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="Modulation-167"><a href="#Modulation-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_double</span> <span class="o">=</span> <span class="n">double</span>
</span><span id="Modulation-168"><a href="#Modulation-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span> <span class="o">=</span> <span class="mi">6</span> <span class="k">if</span> <span class="n">double</span> <span class="k">else</span> <span class="mi">3</span>
</span><span id="Modulation-169"><a href="#Modulation-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span><span id="Modulation-170"><a href="#Modulation-170"><span class="linenos">170</span></a>
</span><span id="Modulation-171"><a href="#Modulation-171"><span class="linenos">171</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="Modulation-172"><a href="#Modulation-172"><span class="linenos">172</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">vec</span><span class="p">))</span>
</span><span id="Modulation-173"><a href="#Modulation-173"><span class="linenos">173</span></a>        <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="Modulation-174"><a href="#Modulation-174"><span class="linenos">174</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="Modulation-175"><a href="#Modulation-175"><span class="linenos">175</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="Modulation-176"><a href="#Modulation-176"><span class="linenos">176</span></a>        <span class="k">return</span> <span class="n">out</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_double</span> <span class="k">else</span> <span class="kc">None</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#Modulation.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#Modulation.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="Modulation.__init__" class="classattr">
                                        <input id="Modulation.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Modulation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">double</span><span class="p">:</span> <span class="nb">bool</span>, </span><span class="param"><span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="Modulation.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Modulation.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Modulation.__init__-165"><a href="#Modulation.__init__-165"><span class="linenos">165</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">double</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Modulation.__init__-166"><a href="#Modulation.__init__-166"><span class="linenos">166</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="Modulation.__init__-167"><a href="#Modulation.__init__-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_double</span> <span class="o">=</span> <span class="n">double</span>
</span><span id="Modulation.__init__-168"><a href="#Modulation.__init__-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span> <span class="o">=</span> <span class="mi">6</span> <span class="k">if</span> <span class="n">double</span> <span class="k">else</span> <span class="mi">3</span>
</span><span id="Modulation.__init__-169"><a href="#Modulation.__init__-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="Modulation.is_double" class="classattr">
                                <div class="attr variable">
            <span class="name">is_double</span>

        
    </div>
    <a class="headerlink" href="#Modulation.is_double"></a>
    
    

                            </div>
                            <div id="Modulation.multiplier" class="classattr">
                                <div class="attr variable">
            <span class="name">multiplier</span>

        
    </div>
    <a class="headerlink" href="#Modulation.multiplier"></a>
    
    

                            </div>
                            <div id="Modulation.lin" class="classattr">
                                <div class="attr variable">
            <span class="name">lin</span>

        
    </div>
    <a class="headerlink" href="#Modulation.lin"></a>
    
    

                            </div>
                            <div id="Modulation.forward" class="classattr">
                                        <input id="Modulation.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Modulation.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Modulation.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Modulation.forward-171"><a href="#Modulation.forward-171"><span class="linenos">171</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="Modulation.forward-172"><a href="#Modulation.forward-172"><span class="linenos">172</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">vec</span><span class="p">))</span>
</span><span id="Modulation.forward-173"><a href="#Modulation.forward-173"><span class="linenos">173</span></a>        <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="Modulation.forward-174"><a href="#Modulation.forward-174"><span class="linenos">174</span></a>            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="Modulation.forward-175"><a href="#Modulation.forward-175"><span class="linenos">175</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multiplier</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="Modulation.forward-176"><a href="#Modulation.forward-176"><span class="linenos">176</span></a>        <span class="k">return</span> <span class="n">out</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_double</span> <span class="k">else</span> <span class="kc">None</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="LastLayer">
                            <input id="LastLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">LastLayer</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="LastLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LastLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LastLayer-179"><a href="#LastLayer-179"><span class="linenos">179</span></a><span class="k">class</span><span class="w"> </span><span class="nc">LastLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="LastLayer-180"><a href="#LastLayer-180"><span class="linenos">180</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LastLayer-181"><a href="#LastLayer-181"><span class="linenos">181</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="LastLayer-182"><a href="#LastLayer-182"><span class="linenos">182</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="LastLayer-183"><a href="#LastLayer-183"><span class="linenos">183</span></a>        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="LastLayer-184"><a href="#LastLayer-184"><span class="linenos">184</span></a>    <span class="p">):</span>
</span><span id="LastLayer-185"><a href="#LastLayer-185"><span class="linenos">185</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="LastLayer-186"><a href="#LastLayer-186"><span class="linenos">186</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="LastLayer-187"><a href="#LastLayer-187"><span class="linenos">187</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="LastLayer-188"><a href="#LastLayer-188"><span class="linenos">188</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adaLN_modulation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</span><span id="LastLayer-189"><a href="#LastLayer-189"><span class="linenos">189</span></a>
</span><span id="LastLayer-190"><a href="#LastLayer-190"><span class="linenos">190</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="LastLayer-191"><a href="#LastLayer-191"><span class="linenos">191</span></a>        <span class="n">mod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adaLN_modulation</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="LastLayer-192"><a href="#LastLayer-192"><span class="linenos">192</span></a>        <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="LastLayer-193"><a href="#LastLayer-193"><span class="linenos">193</span></a>        <span class="k">if</span> <span class="n">shift</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="LastLayer-194"><a href="#LastLayer-194"><span class="linenos">194</span></a>            <span class="n">shift</span> <span class="o">=</span> <span class="n">shift</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="LastLayer-195"><a href="#LastLayer-195"><span class="linenos">195</span></a>            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="LastLayer-196"><a href="#LastLayer-196"><span class="linenos">196</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift</span>
</span><span id="LastLayer-197"><a href="#LastLayer-197"><span class="linenos">197</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="LastLayer-198"><a href="#LastLayer-198"><span class="linenos">198</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#LastLayer.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#LastLayer.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="LastLayer.__init__" class="classattr">
                                        <input id="LastLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">LastLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span></span>)</span>

                <label class="view-source-button" for="LastLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LastLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LastLayer.__init__-180"><a href="#LastLayer.__init__-180"><span class="linenos">180</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="LastLayer.__init__-181"><a href="#LastLayer.__init__-181"><span class="linenos">181</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="LastLayer.__init__-182"><a href="#LastLayer.__init__-182"><span class="linenos">182</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="LastLayer.__init__-183"><a href="#LastLayer.__init__-183"><span class="linenos">183</span></a>        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="LastLayer.__init__-184"><a href="#LastLayer.__init__-184"><span class="linenos">184</span></a>    <span class="p">):</span>
</span><span id="LastLayer.__init__-185"><a href="#LastLayer.__init__-185"><span class="linenos">185</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="LastLayer.__init__-186"><a href="#LastLayer.__init__-186"><span class="linenos">186</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="LastLayer.__init__-187"><a href="#LastLayer.__init__-187"><span class="linenos">187</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="LastLayer.__init__-188"><a href="#LastLayer.__init__-188"><span class="linenos">188</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adaLN_modulation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="LastLayer.norm_final" class="classattr">
                                <div class="attr variable">
            <span class="name">norm_final</span>

        
    </div>
    <a class="headerlink" href="#LastLayer.norm_final"></a>
    
    

                            </div>
                            <div id="LastLayer.linear" class="classattr">
                                <div class="attr variable">
            <span class="name">linear</span>

        
    </div>
    <a class="headerlink" href="#LastLayer.linear"></a>
    
    

                            </div>
                            <div id="LastLayer.adaLN_modulation" class="classattr">
                                <div class="attr variable">
            <span class="name">adaLN_modulation</span>

        
    </div>
    <a class="headerlink" href="#LastLayer.adaLN_modulation"></a>
    
    

                            </div>
                            <div id="LastLayer.forward" class="classattr">
                                        <input id="LastLayer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>, </span><span class="param"><span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="LastLayer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#LastLayer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="LastLayer.forward-190"><a href="#LastLayer.forward-190"><span class="linenos">190</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">vec</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="LastLayer.forward-191"><a href="#LastLayer.forward-191"><span class="linenos">191</span></a>        <span class="n">mod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adaLN_modulation</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span><span id="LastLayer.forward-192"><a href="#LastLayer.forward-192"><span class="linenos">192</span></a>        <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="LastLayer.forward-193"><a href="#LastLayer.forward-193"><span class="linenos">193</span></a>        <span class="k">if</span> <span class="n">shift</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="LastLayer.forward-194"><a href="#LastLayer.forward-194"><span class="linenos">194</span></a>            <span class="n">shift</span> <span class="o">=</span> <span class="n">shift</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="LastLayer.forward-195"><a href="#LastLayer.forward-195"><span class="linenos">195</span></a>            <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="LastLayer.forward-196"><a href="#LastLayer.forward-196"><span class="linenos">196</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift</span>
</span><span id="LastLayer.forward-197"><a href="#LastLayer.forward-197"><span class="linenos">197</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="LastLayer.forward-198"><a href="#LastLayer.forward-198"><span class="linenos">198</span></a>        <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="SingleStreamBlock">
                            <input id="SingleStreamBlock-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">SingleStreamBlock</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="SingleStreamBlock-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingleStreamBlock"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingleStreamBlock-201"><a href="#SingleStreamBlock-201"><span class="linenos">201</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SingleStreamBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="SingleStreamBlock-202"><a href="#SingleStreamBlock-202"><span class="linenos">202</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="SingleStreamBlock-203"><a href="#SingleStreamBlock-203"><span class="linenos">203</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SingleStreamBlock-204"><a href="#SingleStreamBlock-204"><span class="linenos">204</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="SingleStreamBlock-205"><a href="#SingleStreamBlock-205"><span class="linenos">205</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="SingleStreamBlock-206"><a href="#SingleStreamBlock-206"><span class="linenos">206</span></a>        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
</span><span id="SingleStreamBlock-207"><a href="#SingleStreamBlock-207"><span class="linenos">207</span></a>    <span class="p">):</span>
</span><span id="SingleStreamBlock-208"><a href="#SingleStreamBlock-208"><span class="linenos">208</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="SingleStreamBlock-209"><a href="#SingleStreamBlock-209"><span class="linenos">209</span></a>
</span><span id="SingleStreamBlock-210"><a href="#SingleStreamBlock-210"><span class="linenos">210</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="SingleStreamBlock-211"><a href="#SingleStreamBlock-211"><span class="linenos">211</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="SingleStreamBlock-212"><a href="#SingleStreamBlock-212"><span class="linenos">212</span></a>        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
</span><span id="SingleStreamBlock-213"><a href="#SingleStreamBlock-213"><span class="linenos">213</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
</span><span id="SingleStreamBlock-214"><a href="#SingleStreamBlock-214"><span class="linenos">214</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
</span><span id="SingleStreamBlock-215"><a href="#SingleStreamBlock-215"><span class="linenos">215</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="SingleStreamBlock-216"><a href="#SingleStreamBlock-216"><span class="linenos">216</span></a>
</span><span id="SingleStreamBlock-217"><a href="#SingleStreamBlock-217"><span class="linenos">217</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="SingleStreamBlock-218"><a href="#SingleStreamBlock-218"><span class="linenos">218</span></a>            <span class="n">hidden_size</span><span class="p">,</span>
</span><span id="SingleStreamBlock-219"><a href="#SingleStreamBlock-219"><span class="linenos">219</span></a>            <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span>
</span><span id="SingleStreamBlock-220"><a href="#SingleStreamBlock-220"><span class="linenos">220</span></a>            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="SingleStreamBlock-221"><a href="#SingleStreamBlock-221"><span class="linenos">221</span></a>        <span class="p">)</span>
</span><span id="SingleStreamBlock-222"><a href="#SingleStreamBlock-222"><span class="linenos">222</span></a>
</span><span id="SingleStreamBlock-223"><a href="#SingleStreamBlock-223"><span class="linenos">223</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="SingleStreamBlock-224"><a href="#SingleStreamBlock-224"><span class="linenos">224</span></a>
</span><span id="SingleStreamBlock-225"><a href="#SingleStreamBlock-225"><span class="linenos">225</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">QKNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
</span><span id="SingleStreamBlock-226"><a href="#SingleStreamBlock-226"><span class="linenos">226</span></a>
</span><span id="SingleStreamBlock-227"><a href="#SingleStreamBlock-227"><span class="linenos">227</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="SingleStreamBlock-228"><a href="#SingleStreamBlock-228"><span class="linenos">228</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="SingleStreamBlock-229"><a href="#SingleStreamBlock-229"><span class="linenos">229</span></a>
</span><span id="SingleStreamBlock-230"><a href="#SingleStreamBlock-230"><span class="linenos">230</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_act</span> <span class="o">=</span> <span class="n">SiLUActivation</span><span class="p">()</span>
</span><span id="SingleStreamBlock-231"><a href="#SingleStreamBlock-231"><span class="linenos">231</span></a>
</span><span id="SingleStreamBlock-232"><a href="#SingleStreamBlock-232"><span class="linenos">232</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="SingleStreamBlock-233"><a href="#SingleStreamBlock-233"><span class="linenos">233</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SingleStreamBlock-234"><a href="#SingleStreamBlock-234"><span class="linenos">234</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="SingleStreamBlock-235"><a href="#SingleStreamBlock-235"><span class="linenos">235</span></a>        <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="SingleStreamBlock-236"><a href="#SingleStreamBlock-236"><span class="linenos">236</span></a>        <span class="n">mod</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="SingleStreamBlock-237"><a href="#SingleStreamBlock-237"><span class="linenos">237</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="SingleStreamBlock-238"><a href="#SingleStreamBlock-238"><span class="linenos">238</span></a>        <span class="n">mod_shift</span><span class="p">,</span> <span class="n">mod_scale</span><span class="p">,</span> <span class="n">mod_gate</span> <span class="o">=</span> <span class="n">mod</span>  <span class="c1"># type: ignore</span>
</span><span id="SingleStreamBlock-239"><a href="#SingleStreamBlock-239"><span class="linenos">239</span></a>        <span class="n">x_mod</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">mod_scale</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">mod_shift</span>
</span><span id="SingleStreamBlock-240"><a href="#SingleStreamBlock-240"><span class="linenos">240</span></a>
</span><span id="SingleStreamBlock-241"><a href="#SingleStreamBlock-241"><span class="linenos">241</span></a>        <span class="n">qkv</span><span class="p">,</span> <span class="n">mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
</span><span id="SingleStreamBlock-242"><a href="#SingleStreamBlock-242"><span class="linenos">242</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x_mod</span><span class="p">),</span>
</span><span id="SingleStreamBlock-243"><a href="#SingleStreamBlock-243"><span class="linenos">243</span></a>            <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">],</span>
</span><span id="SingleStreamBlock-244"><a href="#SingleStreamBlock-244"><span class="linenos">244</span></a>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="SingleStreamBlock-245"><a href="#SingleStreamBlock-245"><span class="linenos">245</span></a>        <span class="p">)</span>
</span><span id="SingleStreamBlock-246"><a href="#SingleStreamBlock-246"><span class="linenos">246</span></a>
</span><span id="SingleStreamBlock-247"><a href="#SingleStreamBlock-247"><span class="linenos">247</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="SingleStreamBlock-248"><a href="#SingleStreamBlock-248"><span class="linenos">248</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span><span id="SingleStreamBlock-249"><a href="#SingleStreamBlock-249"><span class="linenos">249</span></a>
</span><span id="SingleStreamBlock-250"><a href="#SingleStreamBlock-250"><span class="linenos">250</span></a>        <span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="SingleStreamBlock-251"><a href="#SingleStreamBlock-251"><span class="linenos">251</span></a>
</span><span id="SingleStreamBlock-252"><a href="#SingleStreamBlock-252"><span class="linenos">252</span></a>        <span class="c1"># compute activation in mlp stream, cat again and run second linear layer</span>
</span><span id="SingleStreamBlock-253"><a href="#SingleStreamBlock-253"><span class="linenos">253</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">attn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_act</span><span class="p">(</span><span class="n">mlp</span><span class="p">)),</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="SingleStreamBlock-254"><a href="#SingleStreamBlock-254"><span class="linenos">254</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">mod_gate</span> <span class="o">*</span> <span class="n">output</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#SingleStreamBlock.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#SingleStreamBlock.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="SingleStreamBlock.__init__" class="classattr">
                                        <input id="SingleStreamBlock.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">SingleStreamBlock</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span></span>)</span>

                <label class="view-source-button" for="SingleStreamBlock.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingleStreamBlock.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingleStreamBlock.__init__-202"><a href="#SingleStreamBlock.__init__-202"><span class="linenos">202</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="SingleStreamBlock.__init__-203"><a href="#SingleStreamBlock.__init__-203"><span class="linenos">203</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SingleStreamBlock.__init__-204"><a href="#SingleStreamBlock.__init__-204"><span class="linenos">204</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="SingleStreamBlock.__init__-205"><a href="#SingleStreamBlock.__init__-205"><span class="linenos">205</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="SingleStreamBlock.__init__-206"><a href="#SingleStreamBlock.__init__-206"><span class="linenos">206</span></a>        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
</span><span id="SingleStreamBlock.__init__-207"><a href="#SingleStreamBlock.__init__-207"><span class="linenos">207</span></a>    <span class="p">):</span>
</span><span id="SingleStreamBlock.__init__-208"><a href="#SingleStreamBlock.__init__-208"><span class="linenos">208</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="SingleStreamBlock.__init__-209"><a href="#SingleStreamBlock.__init__-209"><span class="linenos">209</span></a>
</span><span id="SingleStreamBlock.__init__-210"><a href="#SingleStreamBlock.__init__-210"><span class="linenos">210</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="SingleStreamBlock.__init__-211"><a href="#SingleStreamBlock.__init__-211"><span class="linenos">211</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="SingleStreamBlock.__init__-212"><a href="#SingleStreamBlock.__init__-212"><span class="linenos">212</span></a>        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
</span><span id="SingleStreamBlock.__init__-213"><a href="#SingleStreamBlock.__init__-213"><span class="linenos">213</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
</span><span id="SingleStreamBlock.__init__-214"><a href="#SingleStreamBlock.__init__-214"><span class="linenos">214</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
</span><span id="SingleStreamBlock.__init__-215"><a href="#SingleStreamBlock.__init__-215"><span class="linenos">215</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="SingleStreamBlock.__init__-216"><a href="#SingleStreamBlock.__init__-216"><span class="linenos">216</span></a>
</span><span id="SingleStreamBlock.__init__-217"><a href="#SingleStreamBlock.__init__-217"><span class="linenos">217</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="SingleStreamBlock.__init__-218"><a href="#SingleStreamBlock.__init__-218"><span class="linenos">218</span></a>            <span class="n">hidden_size</span><span class="p">,</span>
</span><span id="SingleStreamBlock.__init__-219"><a href="#SingleStreamBlock.__init__-219"><span class="linenos">219</span></a>            <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span>
</span><span id="SingleStreamBlock.__init__-220"><a href="#SingleStreamBlock.__init__-220"><span class="linenos">220</span></a>            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="SingleStreamBlock.__init__-221"><a href="#SingleStreamBlock.__init__-221"><span class="linenos">221</span></a>        <span class="p">)</span>
</span><span id="SingleStreamBlock.__init__-222"><a href="#SingleStreamBlock.__init__-222"><span class="linenos">222</span></a>
</span><span id="SingleStreamBlock.__init__-223"><a href="#SingleStreamBlock.__init__-223"><span class="linenos">223</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="SingleStreamBlock.__init__-224"><a href="#SingleStreamBlock.__init__-224"><span class="linenos">224</span></a>
</span><span id="SingleStreamBlock.__init__-225"><a href="#SingleStreamBlock.__init__-225"><span class="linenos">225</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">QKNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span>
</span><span id="SingleStreamBlock.__init__-226"><a href="#SingleStreamBlock.__init__-226"><span class="linenos">226</span></a>
</span><span id="SingleStreamBlock.__init__-227"><a href="#SingleStreamBlock.__init__-227"><span class="linenos">227</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="SingleStreamBlock.__init__-228"><a href="#SingleStreamBlock.__init__-228"><span class="linenos">228</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="SingleStreamBlock.__init__-229"><a href="#SingleStreamBlock.__init__-229"><span class="linenos">229</span></a>
</span><span id="SingleStreamBlock.__init__-230"><a href="#SingleStreamBlock.__init__-230"><span class="linenos">230</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_act</span> <span class="o">=</span> <span class="n">SiLUActivation</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="SingleStreamBlock.hidden_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_dim</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.hidden_dim"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.num_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_heads</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.num_heads"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.scale" class="classattr">
                                <div class="attr variable">
            <span class="name">scale</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.scale"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.mlp_hidden_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">mlp_hidden_dim</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.mlp_hidden_dim"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.mlp_mult_factor" class="classattr">
                                <div class="attr variable">
            <span class="name">mlp_mult_factor</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.mlp_mult_factor"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.linear1" class="classattr">
                                <div class="attr variable">
            <span class="name">linear1</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.linear1"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.linear2" class="classattr">
                                <div class="attr variable">
            <span class="name">linear2</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.linear2"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.norm" class="classattr">
                                <div class="attr variable">
            <span class="name">norm</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.norm"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.hidden_size"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.pre_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">pre_norm</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.pre_norm"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.mlp_act" class="classattr">
                                <div class="attr variable">
            <span class="name">mlp_act</span>

        
    </div>
    <a class="headerlink" href="#SingleStreamBlock.mlp_act"></a>
    
    

                            </div>
                            <div id="SingleStreamBlock.forward" class="classattr">
                                        <input id="SingleStreamBlock.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pe</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mod</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="SingleStreamBlock.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingleStreamBlock.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingleStreamBlock.forward-232"><a href="#SingleStreamBlock.forward-232"><span class="linenos">232</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="SingleStreamBlock.forward-233"><a href="#SingleStreamBlock.forward-233"><span class="linenos">233</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SingleStreamBlock.forward-234"><a href="#SingleStreamBlock.forward-234"><span class="linenos">234</span></a>        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="SingleStreamBlock.forward-235"><a href="#SingleStreamBlock.forward-235"><span class="linenos">235</span></a>        <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="SingleStreamBlock.forward-236"><a href="#SingleStreamBlock.forward-236"><span class="linenos">236</span></a>        <span class="n">mod</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="SingleStreamBlock.forward-237"><a href="#SingleStreamBlock.forward-237"><span class="linenos">237</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="SingleStreamBlock.forward-238"><a href="#SingleStreamBlock.forward-238"><span class="linenos">238</span></a>        <span class="n">mod_shift</span><span class="p">,</span> <span class="n">mod_scale</span><span class="p">,</span> <span class="n">mod_gate</span> <span class="o">=</span> <span class="n">mod</span>  <span class="c1"># type: ignore</span>
</span><span id="SingleStreamBlock.forward-239"><a href="#SingleStreamBlock.forward-239"><span class="linenos">239</span></a>        <span class="n">x_mod</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">mod_scale</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">mod_shift</span>
</span><span id="SingleStreamBlock.forward-240"><a href="#SingleStreamBlock.forward-240"><span class="linenos">240</span></a>
</span><span id="SingleStreamBlock.forward-241"><a href="#SingleStreamBlock.forward-241"><span class="linenos">241</span></a>        <span class="n">qkv</span><span class="p">,</span> <span class="n">mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
</span><span id="SingleStreamBlock.forward-242"><a href="#SingleStreamBlock.forward-242"><span class="linenos">242</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x_mod</span><span class="p">),</span>
</span><span id="SingleStreamBlock.forward-243"><a href="#SingleStreamBlock.forward-243"><span class="linenos">243</span></a>            <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">],</span>
</span><span id="SingleStreamBlock.forward-244"><a href="#SingleStreamBlock.forward-244"><span class="linenos">244</span></a>            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
</span><span id="SingleStreamBlock.forward-245"><a href="#SingleStreamBlock.forward-245"><span class="linenos">245</span></a>        <span class="p">)</span>
</span><span id="SingleStreamBlock.forward-246"><a href="#SingleStreamBlock.forward-246"><span class="linenos">246</span></a>
</span><span id="SingleStreamBlock.forward-247"><a href="#SingleStreamBlock.forward-247"><span class="linenos">247</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="SingleStreamBlock.forward-248"><a href="#SingleStreamBlock.forward-248"><span class="linenos">248</span></a>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span><span id="SingleStreamBlock.forward-249"><a href="#SingleStreamBlock.forward-249"><span class="linenos">249</span></a>
</span><span id="SingleStreamBlock.forward-250"><a href="#SingleStreamBlock.forward-250"><span class="linenos">250</span></a>        <span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="SingleStreamBlock.forward-251"><a href="#SingleStreamBlock.forward-251"><span class="linenos">251</span></a>
</span><span id="SingleStreamBlock.forward-252"><a href="#SingleStreamBlock.forward-252"><span class="linenos">252</span></a>        <span class="c1"># compute activation in mlp stream, cat again and run second linear layer</span>
</span><span id="SingleStreamBlock.forward-253"><a href="#SingleStreamBlock.forward-253"><span class="linenos">253</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">attn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_act</span><span class="p">(</span><span class="n">mlp</span><span class="p">)),</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="SingleStreamBlock.forward-254"><a href="#SingleStreamBlock.forward-254"><span class="linenos">254</span></a>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">mod_gate</span> <span class="o">*</span> <span class="n">output</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="DoubleStreamBlock">
                            <input id="DoubleStreamBlock-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">DoubleStreamBlock</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="DoubleStreamBlock-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DoubleStreamBlock"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DoubleStreamBlock-257"><a href="#DoubleStreamBlock-257"><span class="linenos">257</span></a><span class="k">class</span><span class="w"> </span><span class="nc">DoubleStreamBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="DoubleStreamBlock-258"><a href="#DoubleStreamBlock-258"><span class="linenos">258</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DoubleStreamBlock-259"><a href="#DoubleStreamBlock-259"><span class="linenos">259</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-260"><a href="#DoubleStreamBlock-260"><span class="linenos">260</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-261"><a href="#DoubleStreamBlock-261"><span class="linenos">261</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-262"><a href="#DoubleStreamBlock-262"><span class="linenos">262</span></a>        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-263"><a href="#DoubleStreamBlock-263"><span class="linenos">263</span></a>    <span class="p">):</span>
</span><span id="DoubleStreamBlock-264"><a href="#DoubleStreamBlock-264"><span class="linenos">264</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="DoubleStreamBlock-265"><a href="#DoubleStreamBlock-265"><span class="linenos">265</span></a>        <span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-266"><a href="#DoubleStreamBlock-266"><span class="linenos">266</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="DoubleStreamBlock-267"><a href="#DoubleStreamBlock-267"><span class="linenos">267</span></a>        <span class="k">assert</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">hidden_size</span><span class="si">=}</span><span class="s2"> must be divisible by </span><span class="si">{</span><span class="n">num_heads</span><span class="si">=}</span><span class="s2">&quot;</span>
</span><span id="DoubleStreamBlock-268"><a href="#DoubleStreamBlock-268"><span class="linenos">268</span></a>
</span><span id="DoubleStreamBlock-269"><a href="#DoubleStreamBlock-269"><span class="linenos">269</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="DoubleStreamBlock-270"><a href="#DoubleStreamBlock-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-271"><a href="#DoubleStreamBlock-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="DoubleStreamBlock-272"><a href="#DoubleStreamBlock-272"><span class="linenos">272</span></a>
</span><span id="DoubleStreamBlock-273"><a href="#DoubleStreamBlock-273"><span class="linenos">273</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span>
</span><span id="DoubleStreamBlock-274"><a href="#DoubleStreamBlock-274"><span class="linenos">274</span></a>            <span class="n">dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-275"><a href="#DoubleStreamBlock-275"><span class="linenos">275</span></a>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-276"><a href="#DoubleStreamBlock-276"><span class="linenos">276</span></a>        <span class="p">)</span>
</span><span id="DoubleStreamBlock-277"><a href="#DoubleStreamBlock-277"><span class="linenos">277</span></a>
</span><span id="DoubleStreamBlock-278"><a href="#DoubleStreamBlock-278"><span class="linenos">278</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-279"><a href="#DoubleStreamBlock-279"><span class="linenos">279</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="DoubleStreamBlock-280"><a href="#DoubleStreamBlock-280"><span class="linenos">280</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="DoubleStreamBlock-281"><a href="#DoubleStreamBlock-281"><span class="linenos">281</span></a>            <span class="n">SiLUActivation</span><span class="p">(),</span>
</span><span id="DoubleStreamBlock-282"><a href="#DoubleStreamBlock-282"><span class="linenos">282</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="DoubleStreamBlock-283"><a href="#DoubleStreamBlock-283"><span class="linenos">283</span></a>        <span class="p">)</span>
</span><span id="DoubleStreamBlock-284"><a href="#DoubleStreamBlock-284"><span class="linenos">284</span></a>
</span><span id="DoubleStreamBlock-285"><a href="#DoubleStreamBlock-285"><span class="linenos">285</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-286"><a href="#DoubleStreamBlock-286"><span class="linenos">286</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span>
</span><span id="DoubleStreamBlock-287"><a href="#DoubleStreamBlock-287"><span class="linenos">287</span></a>            <span class="n">dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-288"><a href="#DoubleStreamBlock-288"><span class="linenos">288</span></a>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-289"><a href="#DoubleStreamBlock-289"><span class="linenos">289</span></a>        <span class="p">)</span>
</span><span id="DoubleStreamBlock-290"><a href="#DoubleStreamBlock-290"><span class="linenos">290</span></a>
</span><span id="DoubleStreamBlock-291"><a href="#DoubleStreamBlock-291"><span class="linenos">291</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-292"><a href="#DoubleStreamBlock-292"><span class="linenos">292</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="DoubleStreamBlock-293"><a href="#DoubleStreamBlock-293"><span class="linenos">293</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="DoubleStreamBlock-294"><a href="#DoubleStreamBlock-294"><span class="linenos">294</span></a>                <span class="n">hidden_size</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-295"><a href="#DoubleStreamBlock-295"><span class="linenos">295</span></a>                <span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-296"><a href="#DoubleStreamBlock-296"><span class="linenos">296</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-297"><a href="#DoubleStreamBlock-297"><span class="linenos">297</span></a>            <span class="p">),</span>
</span><span id="DoubleStreamBlock-298"><a href="#DoubleStreamBlock-298"><span class="linenos">298</span></a>            <span class="n">SiLUActivation</span><span class="p">(),</span>
</span><span id="DoubleStreamBlock-299"><a href="#DoubleStreamBlock-299"><span class="linenos">299</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="DoubleStreamBlock-300"><a href="#DoubleStreamBlock-300"><span class="linenos">300</span></a>        <span class="p">)</span>
</span><span id="DoubleStreamBlock-301"><a href="#DoubleStreamBlock-301"><span class="linenos">301</span></a>
</span><span id="DoubleStreamBlock-302"><a href="#DoubleStreamBlock-302"><span class="linenos">302</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="DoubleStreamBlock-303"><a href="#DoubleStreamBlock-303"><span class="linenos">303</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-304"><a href="#DoubleStreamBlock-304"><span class="linenos">304</span></a>        <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-305"><a href="#DoubleStreamBlock-305"><span class="linenos">305</span></a>        <span class="n">txt</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-306"><a href="#DoubleStreamBlock-306"><span class="linenos">306</span></a>        <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-307"><a href="#DoubleStreamBlock-307"><span class="linenos">307</span></a>        <span class="n">pe_ctx</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock-308"><a href="#DoubleStreamBlock-308"><span class="linenos">308</span></a>        <span class="n">mod_img</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="DoubleStreamBlock-309"><a href="#DoubleStreamBlock-309"><span class="linenos">309</span></a>        <span class="n">mod_txt</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="DoubleStreamBlock-310"><a href="#DoubleStreamBlock-310"><span class="linenos">310</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="DoubleStreamBlock-311"><a href="#DoubleStreamBlock-311"><span class="linenos">311</span></a>        <span class="n">img_mod1</span><span class="p">,</span> <span class="n">img_mod2</span> <span class="o">=</span> <span class="n">mod_img</span>
</span><span id="DoubleStreamBlock-312"><a href="#DoubleStreamBlock-312"><span class="linenos">312</span></a>        <span class="n">txt_mod1</span><span class="p">,</span> <span class="n">txt_mod2</span> <span class="o">=</span> <span class="n">mod_txt</span>
</span><span id="DoubleStreamBlock-313"><a href="#DoubleStreamBlock-313"><span class="linenos">313</span></a>
</span><span id="DoubleStreamBlock-314"><a href="#DoubleStreamBlock-314"><span class="linenos">314</span></a>        <span class="n">img_mod1_shift</span><span class="p">,</span> <span class="n">img_mod1_scale</span><span class="p">,</span> <span class="n">img_mod1_gate</span> <span class="o">=</span> <span class="n">img_mod1</span>
</span><span id="DoubleStreamBlock-315"><a href="#DoubleStreamBlock-315"><span class="linenos">315</span></a>        <span class="n">img_mod2_shift</span><span class="p">,</span> <span class="n">img_mod2_scale</span><span class="p">,</span> <span class="n">img_mod2_gate</span> <span class="o">=</span> <span class="n">img_mod2</span>
</span><span id="DoubleStreamBlock-316"><a href="#DoubleStreamBlock-316"><span class="linenos">316</span></a>        <span class="n">txt_mod1_shift</span><span class="p">,</span> <span class="n">txt_mod1_scale</span><span class="p">,</span> <span class="n">txt_mod1_gate</span> <span class="o">=</span> <span class="n">txt_mod1</span>
</span><span id="DoubleStreamBlock-317"><a href="#DoubleStreamBlock-317"><span class="linenos">317</span></a>        <span class="n">txt_mod2_shift</span><span class="p">,</span> <span class="n">txt_mod2_scale</span><span class="p">,</span> <span class="n">txt_mod2_gate</span> <span class="o">=</span> <span class="n">txt_mod2</span>
</span><span id="DoubleStreamBlock-318"><a href="#DoubleStreamBlock-318"><span class="linenos">318</span></a>
</span><span id="DoubleStreamBlock-319"><a href="#DoubleStreamBlock-319"><span class="linenos">319</span></a>        <span class="c1"># prepare image for attention</span>
</span><span id="DoubleStreamBlock-320"><a href="#DoubleStreamBlock-320"><span class="linenos">320</span></a>        <span class="n">img_modulated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_norm1</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-321"><a href="#DoubleStreamBlock-321"><span class="linenos">321</span></a>        <span class="n">img_modulated</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">img_mod1_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">img_modulated</span> <span class="o">+</span> <span class="n">img_mod1_shift</span>
</span><span id="DoubleStreamBlock-322"><a href="#DoubleStreamBlock-322"><span class="linenos">322</span></a>
</span><span id="DoubleStreamBlock-323"><a href="#DoubleStreamBlock-323"><span class="linenos">323</span></a>        <span class="n">img_qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">img_modulated</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-324"><a href="#DoubleStreamBlock-324"><span class="linenos">324</span></a>        <span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span><span class="p">,</span> <span class="n">img_v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">img_qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-325"><a href="#DoubleStreamBlock-325"><span class="linenos">325</span></a>        <span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span><span class="p">,</span> <span class="n">img_v</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-326"><a href="#DoubleStreamBlock-326"><span class="linenos">326</span></a>
</span><span id="DoubleStreamBlock-327"><a href="#DoubleStreamBlock-327"><span class="linenos">327</span></a>        <span class="c1"># prepare txt for attention</span>
</span><span id="DoubleStreamBlock-328"><a href="#DoubleStreamBlock-328"><span class="linenos">328</span></a>        <span class="n">txt_modulated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm1</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-329"><a href="#DoubleStreamBlock-329"><span class="linenos">329</span></a>        <span class="n">txt_modulated</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">txt_mod1_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">txt_modulated</span> <span class="o">+</span> <span class="n">txt_mod1_shift</span>
</span><span id="DoubleStreamBlock-330"><a href="#DoubleStreamBlock-330"><span class="linenos">330</span></a>
</span><span id="DoubleStreamBlock-331"><a href="#DoubleStreamBlock-331"><span class="linenos">331</span></a>        <span class="n">txt_qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">txt_modulated</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-332"><a href="#DoubleStreamBlock-332"><span class="linenos">332</span></a>        <span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span><span class="p">,</span> <span class="n">txt_v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">txt_qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-333"><a href="#DoubleStreamBlock-333"><span class="linenos">333</span></a>        <span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span><span class="p">,</span> <span class="n">txt_v</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-334"><a href="#DoubleStreamBlock-334"><span class="linenos">334</span></a>
</span><span id="DoubleStreamBlock-335"><a href="#DoubleStreamBlock-335"><span class="linenos">335</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_q</span><span class="p">,</span> <span class="n">img_q</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-336"><a href="#DoubleStreamBlock-336"><span class="linenos">336</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_k</span><span class="p">,</span> <span class="n">img_k</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-337"><a href="#DoubleStreamBlock-337"><span class="linenos">337</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_v</span><span class="p">,</span> <span class="n">img_v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-338"><a href="#DoubleStreamBlock-338"><span class="linenos">338</span></a>
</span><span id="DoubleStreamBlock-339"><a href="#DoubleStreamBlock-339"><span class="linenos">339</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pe_ctx</span><span class="p">,</span> <span class="n">pe</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-340"><a href="#DoubleStreamBlock-340"><span class="linenos">340</span></a>        <span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-341"><a href="#DoubleStreamBlock-341"><span class="linenos">341</span></a>        <span class="n">txt_attn</span><span class="p">,</span> <span class="n">img_attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">txt_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">attn</span><span class="p">[:,</span> <span class="n">txt_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">:]</span>
</span><span id="DoubleStreamBlock-342"><a href="#DoubleStreamBlock-342"><span class="linenos">342</span></a>
</span><span id="DoubleStreamBlock-343"><a href="#DoubleStreamBlock-343"><span class="linenos">343</span></a>        <span class="c1"># calculate the img blocks</span>
</span><span id="DoubleStreamBlock-344"><a href="#DoubleStreamBlock-344"><span class="linenos">344</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">+</span> <span class="n">img_mod1_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">img_attn</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-345"><a href="#DoubleStreamBlock-345"><span class="linenos">345</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">+</span> <span class="n">img_mod2_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_mlp</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">img_mod2_scale</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_norm2</span><span class="p">(</span><span class="n">img</span><span class="p">))</span> <span class="o">+</span> <span class="n">img_mod2_shift</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-346"><a href="#DoubleStreamBlock-346"><span class="linenos">346</span></a>
</span><span id="DoubleStreamBlock-347"><a href="#DoubleStreamBlock-347"><span class="linenos">347</span></a>        <span class="c1"># calculate the txt blocks</span>
</span><span id="DoubleStreamBlock-348"><a href="#DoubleStreamBlock-348"><span class="linenos">348</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span> <span class="o">+</span> <span class="n">txt_mod1_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">txt_attn</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-349"><a href="#DoubleStreamBlock-349"><span class="linenos">349</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span> <span class="o">+</span> <span class="n">txt_mod2_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_mlp</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">txt_mod2_scale</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">txt_norm2</span><span class="p">(</span><span class="n">txt</span><span class="p">))</span> <span class="o">+</span> <span class="n">txt_mod2_shift</span><span class="p">)</span>
</span><span id="DoubleStreamBlock-350"><a href="#DoubleStreamBlock-350"><span class="linenos">350</span></a>        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">txt</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#DoubleStreamBlock.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#DoubleStreamBlock.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="DoubleStreamBlock.__init__" class="classattr">
                                        <input id="DoubleStreamBlock.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">DoubleStreamBlock</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span></span>)</span>

                <label class="view-source-button" for="DoubleStreamBlock.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DoubleStreamBlock.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DoubleStreamBlock.__init__-258"><a href="#DoubleStreamBlock.__init__-258"><span class="linenos">258</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DoubleStreamBlock.__init__-259"><a href="#DoubleStreamBlock.__init__-259"><span class="linenos">259</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-260"><a href="#DoubleStreamBlock.__init__-260"><span class="linenos">260</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-261"><a href="#DoubleStreamBlock.__init__-261"><span class="linenos">261</span></a>        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-262"><a href="#DoubleStreamBlock.__init__-262"><span class="linenos">262</span></a>        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-263"><a href="#DoubleStreamBlock.__init__-263"><span class="linenos">263</span></a>    <span class="p">):</span>
</span><span id="DoubleStreamBlock.__init__-264"><a href="#DoubleStreamBlock.__init__-264"><span class="linenos">264</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="DoubleStreamBlock.__init__-265"><a href="#DoubleStreamBlock.__init__-265"><span class="linenos">265</span></a>        <span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-266"><a href="#DoubleStreamBlock.__init__-266"><span class="linenos">266</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span><span id="DoubleStreamBlock.__init__-267"><a href="#DoubleStreamBlock.__init__-267"><span class="linenos">267</span></a>        <span class="k">assert</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">hidden_size</span><span class="si">=}</span><span class="s2"> must be divisible by </span><span class="si">{</span><span class="n">num_heads</span><span class="si">=}</span><span class="s2">&quot;</span>
</span><span id="DoubleStreamBlock.__init__-268"><a href="#DoubleStreamBlock.__init__-268"><span class="linenos">268</span></a>
</span><span id="DoubleStreamBlock.__init__-269"><a href="#DoubleStreamBlock.__init__-269"><span class="linenos">269</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="DoubleStreamBlock.__init__-270"><a href="#DoubleStreamBlock.__init__-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-271"><a href="#DoubleStreamBlock.__init__-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span> <span class="o">=</span> <span class="mi">2</span>
</span><span id="DoubleStreamBlock.__init__-272"><a href="#DoubleStreamBlock.__init__-272"><span class="linenos">272</span></a>
</span><span id="DoubleStreamBlock.__init__-273"><a href="#DoubleStreamBlock.__init__-273"><span class="linenos">273</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span>
</span><span id="DoubleStreamBlock.__init__-274"><a href="#DoubleStreamBlock.__init__-274"><span class="linenos">274</span></a>            <span class="n">dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-275"><a href="#DoubleStreamBlock.__init__-275"><span class="linenos">275</span></a>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-276"><a href="#DoubleStreamBlock.__init__-276"><span class="linenos">276</span></a>        <span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-277"><a href="#DoubleStreamBlock.__init__-277"><span class="linenos">277</span></a>
</span><span id="DoubleStreamBlock.__init__-278"><a href="#DoubleStreamBlock.__init__-278"><span class="linenos">278</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-279"><a href="#DoubleStreamBlock.__init__-279"><span class="linenos">279</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">img_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="DoubleStreamBlock.__init__-280"><a href="#DoubleStreamBlock.__init__-280"><span class="linenos">280</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="DoubleStreamBlock.__init__-281"><a href="#DoubleStreamBlock.__init__-281"><span class="linenos">281</span></a>            <span class="n">SiLUActivation</span><span class="p">(),</span>
</span><span id="DoubleStreamBlock.__init__-282"><a href="#DoubleStreamBlock.__init__-282"><span class="linenos">282</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="DoubleStreamBlock.__init__-283"><a href="#DoubleStreamBlock.__init__-283"><span class="linenos">283</span></a>        <span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-284"><a href="#DoubleStreamBlock.__init__-284"><span class="linenos">284</span></a>
</span><span id="DoubleStreamBlock.__init__-285"><a href="#DoubleStreamBlock.__init__-285"><span class="linenos">285</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-286"><a href="#DoubleStreamBlock.__init__-286"><span class="linenos">286</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span>
</span><span id="DoubleStreamBlock.__init__-287"><a href="#DoubleStreamBlock.__init__-287"><span class="linenos">287</span></a>            <span class="n">dim</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-288"><a href="#DoubleStreamBlock.__init__-288"><span class="linenos">288</span></a>            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-289"><a href="#DoubleStreamBlock.__init__-289"><span class="linenos">289</span></a>        <span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-290"><a href="#DoubleStreamBlock.__init__-290"><span class="linenos">290</span></a>
</span><span id="DoubleStreamBlock.__init__-291"><a href="#DoubleStreamBlock.__init__-291"><span class="linenos">291</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.__init__-292"><a href="#DoubleStreamBlock.__init__-292"><span class="linenos">292</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">txt_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span><span id="DoubleStreamBlock.__init__-293"><a href="#DoubleStreamBlock.__init__-293"><span class="linenos">293</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span><span id="DoubleStreamBlock.__init__-294"><a href="#DoubleStreamBlock.__init__-294"><span class="linenos">294</span></a>                <span class="n">hidden_size</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-295"><a href="#DoubleStreamBlock.__init__-295"><span class="linenos">295</span></a>                <span class="n">mlp_hidden_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_mult_factor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-296"><a href="#DoubleStreamBlock.__init__-296"><span class="linenos">296</span></a>                <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.__init__-297"><a href="#DoubleStreamBlock.__init__-297"><span class="linenos">297</span></a>            <span class="p">),</span>
</span><span id="DoubleStreamBlock.__init__-298"><a href="#DoubleStreamBlock.__init__-298"><span class="linenos">298</span></a>            <span class="n">SiLUActivation</span><span class="p">(),</span>
</span><span id="DoubleStreamBlock.__init__-299"><a href="#DoubleStreamBlock.__init__-299"><span class="linenos">299</span></a>            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span><span id="DoubleStreamBlock.__init__-300"><a href="#DoubleStreamBlock.__init__-300"><span class="linenos">300</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="DoubleStreamBlock.num_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_heads</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.num_heads"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.hidden_size"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.img_norm1" class="classattr">
                                <div class="attr variable">
            <span class="name">img_norm1</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.img_norm1"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.mlp_mult_factor" class="classattr">
                                <div class="attr variable">
            <span class="name">mlp_mult_factor</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.mlp_mult_factor"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.img_attn" class="classattr">
                                <div class="attr variable">
            <span class="name">img_attn</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.img_attn"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.img_norm2" class="classattr">
                                <div class="attr variable">
            <span class="name">img_norm2</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.img_norm2"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.img_mlp" class="classattr">
                                <div class="attr variable">
            <span class="name">img_mlp</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.img_mlp"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.txt_norm1" class="classattr">
                                <div class="attr variable">
            <span class="name">txt_norm1</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.txt_norm1"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.txt_attn" class="classattr">
                                <div class="attr variable">
            <span class="name">txt_attn</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.txt_attn"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.txt_norm2" class="classattr">
                                <div class="attr variable">
            <span class="name">txt_norm2</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.txt_norm2"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.txt_mlp" class="classattr">
                                <div class="attr variable">
            <span class="name">txt_mlp</span>

        
    </div>
    <a class="headerlink" href="#DoubleStreamBlock.txt_mlp"></a>
    
    

                            </div>
                            <div id="DoubleStreamBlock.forward" class="classattr">
                                        <input id="DoubleStreamBlock.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">img</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">txt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pe</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pe_ctx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mod_img</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>,</span><span class="param">	<span class="n">mod_txt</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="DoubleStreamBlock.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DoubleStreamBlock.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DoubleStreamBlock.forward-302"><a href="#DoubleStreamBlock.forward-302"><span class="linenos">302</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
</span><span id="DoubleStreamBlock.forward-303"><a href="#DoubleStreamBlock.forward-303"><span class="linenos">303</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.forward-304"><a href="#DoubleStreamBlock.forward-304"><span class="linenos">304</span></a>        <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.forward-305"><a href="#DoubleStreamBlock.forward-305"><span class="linenos">305</span></a>        <span class="n">txt</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.forward-306"><a href="#DoubleStreamBlock.forward-306"><span class="linenos">306</span></a>        <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.forward-307"><a href="#DoubleStreamBlock.forward-307"><span class="linenos">307</span></a>        <span class="n">pe_ctx</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
</span><span id="DoubleStreamBlock.forward-308"><a href="#DoubleStreamBlock.forward-308"><span class="linenos">308</span></a>        <span class="n">mod_img</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="DoubleStreamBlock.forward-309"><a href="#DoubleStreamBlock.forward-309"><span class="linenos">309</span></a>        <span class="n">mod_txt</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
</span><span id="DoubleStreamBlock.forward-310"><a href="#DoubleStreamBlock.forward-310"><span class="linenos">310</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="DoubleStreamBlock.forward-311"><a href="#DoubleStreamBlock.forward-311"><span class="linenos">311</span></a>        <span class="n">img_mod1</span><span class="p">,</span> <span class="n">img_mod2</span> <span class="o">=</span> <span class="n">mod_img</span>
</span><span id="DoubleStreamBlock.forward-312"><a href="#DoubleStreamBlock.forward-312"><span class="linenos">312</span></a>        <span class="n">txt_mod1</span><span class="p">,</span> <span class="n">txt_mod2</span> <span class="o">=</span> <span class="n">mod_txt</span>
</span><span id="DoubleStreamBlock.forward-313"><a href="#DoubleStreamBlock.forward-313"><span class="linenos">313</span></a>
</span><span id="DoubleStreamBlock.forward-314"><a href="#DoubleStreamBlock.forward-314"><span class="linenos">314</span></a>        <span class="n">img_mod1_shift</span><span class="p">,</span> <span class="n">img_mod1_scale</span><span class="p">,</span> <span class="n">img_mod1_gate</span> <span class="o">=</span> <span class="n">img_mod1</span>
</span><span id="DoubleStreamBlock.forward-315"><a href="#DoubleStreamBlock.forward-315"><span class="linenos">315</span></a>        <span class="n">img_mod2_shift</span><span class="p">,</span> <span class="n">img_mod2_scale</span><span class="p">,</span> <span class="n">img_mod2_gate</span> <span class="o">=</span> <span class="n">img_mod2</span>
</span><span id="DoubleStreamBlock.forward-316"><a href="#DoubleStreamBlock.forward-316"><span class="linenos">316</span></a>        <span class="n">txt_mod1_shift</span><span class="p">,</span> <span class="n">txt_mod1_scale</span><span class="p">,</span> <span class="n">txt_mod1_gate</span> <span class="o">=</span> <span class="n">txt_mod1</span>
</span><span id="DoubleStreamBlock.forward-317"><a href="#DoubleStreamBlock.forward-317"><span class="linenos">317</span></a>        <span class="n">txt_mod2_shift</span><span class="p">,</span> <span class="n">txt_mod2_scale</span><span class="p">,</span> <span class="n">txt_mod2_gate</span> <span class="o">=</span> <span class="n">txt_mod2</span>
</span><span id="DoubleStreamBlock.forward-318"><a href="#DoubleStreamBlock.forward-318"><span class="linenos">318</span></a>
</span><span id="DoubleStreamBlock.forward-319"><a href="#DoubleStreamBlock.forward-319"><span class="linenos">319</span></a>        <span class="c1"># prepare image for attention</span>
</span><span id="DoubleStreamBlock.forward-320"><a href="#DoubleStreamBlock.forward-320"><span class="linenos">320</span></a>        <span class="n">img_modulated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_norm1</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-321"><a href="#DoubleStreamBlock.forward-321"><span class="linenos">321</span></a>        <span class="n">img_modulated</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">img_mod1_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">img_modulated</span> <span class="o">+</span> <span class="n">img_mod1_shift</span>
</span><span id="DoubleStreamBlock.forward-322"><a href="#DoubleStreamBlock.forward-322"><span class="linenos">322</span></a>
</span><span id="DoubleStreamBlock.forward-323"><a href="#DoubleStreamBlock.forward-323"><span class="linenos">323</span></a>        <span class="n">img_qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">img_modulated</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-324"><a href="#DoubleStreamBlock.forward-324"><span class="linenos">324</span></a>        <span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span><span class="p">,</span> <span class="n">img_v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">img_qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-325"><a href="#DoubleStreamBlock.forward-325"><span class="linenos">325</span></a>        <span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">img_q</span><span class="p">,</span> <span class="n">img_k</span><span class="p">,</span> <span class="n">img_v</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-326"><a href="#DoubleStreamBlock.forward-326"><span class="linenos">326</span></a>
</span><span id="DoubleStreamBlock.forward-327"><a href="#DoubleStreamBlock.forward-327"><span class="linenos">327</span></a>        <span class="c1"># prepare txt for attention</span>
</span><span id="DoubleStreamBlock.forward-328"><a href="#DoubleStreamBlock.forward-328"><span class="linenos">328</span></a>        <span class="n">txt_modulated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_norm1</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-329"><a href="#DoubleStreamBlock.forward-329"><span class="linenos">329</span></a>        <span class="n">txt_modulated</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">txt_mod1_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">txt_modulated</span> <span class="o">+</span> <span class="n">txt_mod1_shift</span>
</span><span id="DoubleStreamBlock.forward-330"><a href="#DoubleStreamBlock.forward-330"><span class="linenos">330</span></a>
</span><span id="DoubleStreamBlock.forward-331"><a href="#DoubleStreamBlock.forward-331"><span class="linenos">331</span></a>        <span class="n">txt_qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">txt_modulated</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-332"><a href="#DoubleStreamBlock.forward-332"><span class="linenos">332</span></a>        <span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span><span class="p">,</span> <span class="n">txt_v</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">txt_qkv</span><span class="p">,</span> <span class="s2">&quot;B L (K H D) -&gt; K B H L D&quot;</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-333"><a href="#DoubleStreamBlock.forward-333"><span class="linenos">333</span></a>        <span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">txt_q</span><span class="p">,</span> <span class="n">txt_k</span><span class="p">,</span> <span class="n">txt_v</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-334"><a href="#DoubleStreamBlock.forward-334"><span class="linenos">334</span></a>
</span><span id="DoubleStreamBlock.forward-335"><a href="#DoubleStreamBlock.forward-335"><span class="linenos">335</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_q</span><span class="p">,</span> <span class="n">img_q</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-336"><a href="#DoubleStreamBlock.forward-336"><span class="linenos">336</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_k</span><span class="p">,</span> <span class="n">img_k</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-337"><a href="#DoubleStreamBlock.forward-337"><span class="linenos">337</span></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">txt_v</span><span class="p">,</span> <span class="n">img_v</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-338"><a href="#DoubleStreamBlock.forward-338"><span class="linenos">338</span></a>
</span><span id="DoubleStreamBlock.forward-339"><a href="#DoubleStreamBlock.forward-339"><span class="linenos">339</span></a>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pe_ctx</span><span class="p">,</span> <span class="n">pe</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-340"><a href="#DoubleStreamBlock.forward-340"><span class="linenos">340</span></a>        <span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-341"><a href="#DoubleStreamBlock.forward-341"><span class="linenos">341</span></a>        <span class="n">txt_attn</span><span class="p">,</span> <span class="n">img_attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">txt_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">attn</span><span class="p">[:,</span> <span class="n">txt_q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">:]</span>
</span><span id="DoubleStreamBlock.forward-342"><a href="#DoubleStreamBlock.forward-342"><span class="linenos">342</span></a>
</span><span id="DoubleStreamBlock.forward-343"><a href="#DoubleStreamBlock.forward-343"><span class="linenos">343</span></a>        <span class="c1"># calculate the img blocks</span>
</span><span id="DoubleStreamBlock.forward-344"><a href="#DoubleStreamBlock.forward-344"><span class="linenos">344</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">+</span> <span class="n">img_mod1_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_attn</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">img_attn</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-345"><a href="#DoubleStreamBlock.forward-345"><span class="linenos">345</span></a>        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">+</span> <span class="n">img_mod2_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_mlp</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">img_mod2_scale</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_norm2</span><span class="p">(</span><span class="n">img</span><span class="p">))</span> <span class="o">+</span> <span class="n">img_mod2_shift</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-346"><a href="#DoubleStreamBlock.forward-346"><span class="linenos">346</span></a>
</span><span id="DoubleStreamBlock.forward-347"><a href="#DoubleStreamBlock.forward-347"><span class="linenos">347</span></a>        <span class="c1"># calculate the txt blocks</span>
</span><span id="DoubleStreamBlock.forward-348"><a href="#DoubleStreamBlock.forward-348"><span class="linenos">348</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span> <span class="o">+</span> <span class="n">txt_mod1_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_attn</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">txt_attn</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-349"><a href="#DoubleStreamBlock.forward-349"><span class="linenos">349</span></a>        <span class="n">txt</span> <span class="o">=</span> <span class="n">txt</span> <span class="o">+</span> <span class="n">txt_mod2_gate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">txt_mlp</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">txt_mod2_scale</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">txt_norm2</span><span class="p">(</span><span class="n">txt</span><span class="p">))</span> <span class="o">+</span> <span class="n">txt_mod2_shift</span><span class="p">)</span>
</span><span id="DoubleStreamBlock.forward-350"><a href="#DoubleStreamBlock.forward-350"><span class="linenos">350</span></a>        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">txt</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="MLPEmbedder">
                            <input id="MLPEmbedder-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MLPEmbedder</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="MLPEmbedder-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLPEmbedder"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLPEmbedder-353"><a href="#MLPEmbedder-353"><span class="linenos">353</span></a><span class="k">class</span><span class="w"> </span><span class="nc">MLPEmbedder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="MLPEmbedder-354"><a href="#MLPEmbedder-354"><span class="linenos">354</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="MLPEmbedder-355"><a href="#MLPEmbedder-355"><span class="linenos">355</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MLPEmbedder-356"><a href="#MLPEmbedder-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span><span id="MLPEmbedder-357"><a href="#MLPEmbedder-357"><span class="linenos">357</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">silu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
</span><span id="MLPEmbedder-358"><a href="#MLPEmbedder-358"><span class="linenos">358</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span><span id="MLPEmbedder-359"><a href="#MLPEmbedder-359"><span class="linenos">359</span></a>
</span><span id="MLPEmbedder-360"><a href="#MLPEmbedder-360"><span class="linenos">360</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="MLPEmbedder-361"><a href="#MLPEmbedder-361"><span class="linenos">361</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#MLPEmbedder.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#MLPEmbedder.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="MLPEmbedder.__init__" class="classattr">
                                        <input id="MLPEmbedder.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MLPEmbedder</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="MLPEmbedder.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLPEmbedder.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLPEmbedder.__init__-354"><a href="#MLPEmbedder.__init__-354"><span class="linenos">354</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">disable_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="MLPEmbedder.__init__-355"><a href="#MLPEmbedder.__init__-355"><span class="linenos">355</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="MLPEmbedder.__init__-356"><a href="#MLPEmbedder.__init__-356"><span class="linenos">356</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span><span id="MLPEmbedder.__init__-357"><a href="#MLPEmbedder.__init__-357"><span class="linenos">357</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">silu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">()</span>
</span><span id="MLPEmbedder.__init__-358"><a href="#MLPEmbedder.__init__-358"><span class="linenos">358</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="n">disable_bias</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="MLPEmbedder.in_layer" class="classattr">
                                <div class="attr variable">
            <span class="name">in_layer</span>

        
    </div>
    <a class="headerlink" href="#MLPEmbedder.in_layer"></a>
    
    

                            </div>
                            <div id="MLPEmbedder.silu" class="classattr">
                                <div class="attr variable">
            <span class="name">silu</span>

        
    </div>
    <a class="headerlink" href="#MLPEmbedder.silu"></a>
    
    

                            </div>
                            <div id="MLPEmbedder.out_layer" class="classattr">
                                <div class="attr variable">
            <span class="name">out_layer</span>

        
    </div>
    <a class="headerlink" href="#MLPEmbedder.out_layer"></a>
    
    

                            </div>
                            <div id="MLPEmbedder.forward" class="classattr">
                                        <input id="MLPEmbedder.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="MLPEmbedder.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MLPEmbedder.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MLPEmbedder.forward-360"><a href="#MLPEmbedder.forward-360"><span class="linenos">360</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="MLPEmbedder.forward-361"><a href="#MLPEmbedder.forward-361"><span class="linenos">361</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="EmbedND">
                            <input id="EmbedND-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">EmbedND</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="EmbedND-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EmbedND"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EmbedND-364"><a href="#EmbedND-364"><span class="linenos">364</span></a><span class="k">class</span><span class="w"> </span><span class="nc">EmbedND</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="EmbedND-365"><a href="#EmbedND-365"><span class="linenos">365</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">axes_dim</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
</span><span id="EmbedND-366"><a href="#EmbedND-366"><span class="linenos">366</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="EmbedND-367"><a href="#EmbedND-367"><span class="linenos">367</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
</span><span id="EmbedND-368"><a href="#EmbedND-368"><span class="linenos">368</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span>
</span><span id="EmbedND-369"><a href="#EmbedND-369"><span class="linenos">369</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span> <span class="o">=</span> <span class="n">axes_dim</span>
</span><span id="EmbedND-370"><a href="#EmbedND-370"><span class="linenos">370</span></a>
</span><span id="EmbedND-371"><a href="#EmbedND-371"><span class="linenos">371</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="EmbedND-372"><a href="#EmbedND-372"><span class="linenos">372</span></a>        <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span><span id="EmbedND-373"><a href="#EmbedND-373"><span class="linenos">373</span></a>            <span class="p">[</span><span class="n">rope</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">))],</span>
</span><span id="EmbedND-374"><a href="#EmbedND-374"><span class="linenos">374</span></a>            <span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span>
</span><span id="EmbedND-375"><a href="#EmbedND-375"><span class="linenos">375</span></a>        <span class="p">)</span>
</span><span id="EmbedND-376"><a href="#EmbedND-376"><span class="linenos">376</span></a>
</span><span id="EmbedND-377"><a href="#EmbedND-377"><span class="linenos">377</span></a>        <span class="k">return</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#EmbedND.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#EmbedND.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="EmbedND.__init__" class="classattr">
                                        <input id="EmbedND.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">EmbedND</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">theta</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">axes_dim</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span></span>)</span>

                <label class="view-source-button" for="EmbedND.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EmbedND.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EmbedND.__init__-365"><a href="#EmbedND.__init__-365"><span class="linenos">365</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">axes_dim</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
</span><span id="EmbedND.__init__-366"><a href="#EmbedND.__init__-366"><span class="linenos">366</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="EmbedND.__init__-367"><a href="#EmbedND.__init__-367"><span class="linenos">367</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
</span><span id="EmbedND.__init__-368"><a href="#EmbedND.__init__-368"><span class="linenos">368</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span>
</span><span id="EmbedND.__init__-369"><a href="#EmbedND.__init__-369"><span class="linenos">369</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span> <span class="o">=</span> <span class="n">axes_dim</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="EmbedND.dim" class="classattr">
                                <div class="attr variable">
            <span class="name">dim</span>

        
    </div>
    <a class="headerlink" href="#EmbedND.dim"></a>
    
    

                            </div>
                            <div id="EmbedND.theta" class="classattr">
                                <div class="attr variable">
            <span class="name">theta</span>

        
    </div>
    <a class="headerlink" href="#EmbedND.theta"></a>
    
    

                            </div>
                            <div id="EmbedND.axes_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">axes_dim</span>

        
    </div>
    <a class="headerlink" href="#EmbedND.axes_dim"></a>
    
    

                            </div>
                            <div id="EmbedND.forward" class="classattr">
                                        <input id="EmbedND.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="EmbedND.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EmbedND.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EmbedND.forward-371"><a href="#EmbedND.forward-371"><span class="linenos">371</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="EmbedND.forward-372"><a href="#EmbedND.forward-372"><span class="linenos">372</span></a>        <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span><span id="EmbedND.forward-373"><a href="#EmbedND.forward-373"><span class="linenos">373</span></a>            <span class="p">[</span><span class="n">rope</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axes_dim</span><span class="p">))],</span>
</span><span id="EmbedND.forward-374"><a href="#EmbedND.forward-374"><span class="linenos">374</span></a>            <span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">,</span>
</span><span id="EmbedND.forward-375"><a href="#EmbedND.forward-375"><span class="linenos">375</span></a>        <span class="p">)</span>
</span><span id="EmbedND.forward-376"><a href="#EmbedND.forward-376"><span class="linenos">376</span></a>
</span><span id="EmbedND.forward-377"><a href="#EmbedND.forward-377"><span class="linenos">377</span></a>        <span class="k">return</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="timestep_embedding">
                            <input id="timestep_embedding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">timestep_embedding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>, </span><span class="param"><span class="n">dim</span>, </span><span class="param"><span class="n">max_period</span><span class="o">=</span><span class="mi">10000</span>, </span><span class="param"><span class="n">time_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1000.0</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="timestep_embedding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#timestep_embedding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="timestep_embedding-380"><a href="#timestep_embedding-380"><span class="linenos">380</span></a><span class="k">def</span><span class="w"> </span><span class="nf">timestep_embedding</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">max_period</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">time_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1000.0</span><span class="p">):</span>
</span><span id="timestep_embedding-381"><a href="#timestep_embedding-381"><span class="linenos">381</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="timestep_embedding-382"><a href="#timestep_embedding-382"><span class="linenos">382</span></a><span class="sd">    Create sinusoidal timestep embeddings.</span>
</span><span id="timestep_embedding-383"><a href="#timestep_embedding-383"><span class="linenos">383</span></a><span class="sd">    :param t: a 1-D Tensor of N indices, one per batch element.</span>
</span><span id="timestep_embedding-384"><a href="#timestep_embedding-384"><span class="linenos">384</span></a><span class="sd">                      These may be fractional.</span>
</span><span id="timestep_embedding-385"><a href="#timestep_embedding-385"><span class="linenos">385</span></a><span class="sd">    :param dim: the dimension of the output.</span>
</span><span id="timestep_embedding-386"><a href="#timestep_embedding-386"><span class="linenos">386</span></a><span class="sd">    :param max_period: controls the minimum frequency of the embeddings.</span>
</span><span id="timestep_embedding-387"><a href="#timestep_embedding-387"><span class="linenos">387</span></a><span class="sd">    :return: an (N, D) Tensor of positional embeddings.</span>
</span><span id="timestep_embedding-388"><a href="#timestep_embedding-388"><span class="linenos">388</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="timestep_embedding-389"><a href="#timestep_embedding-389"><span class="linenos">389</span></a>    <span class="n">t</span> <span class="o">=</span> <span class="n">time_factor</span> <span class="o">*</span> <span class="n">t</span>
</span><span id="timestep_embedding-390"><a href="#timestep_embedding-390"><span class="linenos">390</span></a>    <span class="n">half</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
</span><span id="timestep_embedding-391"><a href="#timestep_embedding-391"><span class="linenos">391</span></a>    <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
</span><span id="timestep_embedding-392"><a href="#timestep_embedding-392"><span class="linenos">392</span></a>        <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">max_period</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">half</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="n">half</span>  <span class="c1"># float32 originally</span>
</span><span id="timestep_embedding-393"><a href="#timestep_embedding-393"><span class="linenos">393</span></a>    <span class="p">)</span>
</span><span id="timestep_embedding-394"><a href="#timestep_embedding-394"><span class="linenos">394</span></a>
</span><span id="timestep_embedding-395"><a href="#timestep_embedding-395"><span class="linenos">395</span></a>    <span class="n">args</span> <span class="o">=</span> <span class="n">t</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="n">freqs</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>
</span><span id="timestep_embedding-396"><a href="#timestep_embedding-396"><span class="linenos">396</span></a>    <span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">args</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="timestep_embedding-397"><a href="#timestep_embedding-397"><span class="linenos">397</span></a>    <span class="k">if</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span><span class="p">:</span>
</span><span id="timestep_embedding-398"><a href="#timestep_embedding-398"><span class="linenos">398</span></a>        <span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">embedding</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">embedding</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="timestep_embedding-399"><a href="#timestep_embedding-399"><span class="linenos">399</span></a>    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
</span><span id="timestep_embedding-400"><a href="#timestep_embedding-400"><span class="linenos">400</span></a>        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span><span id="timestep_embedding-401"><a href="#timestep_embedding-401"><span class="linenos">401</span></a>    <span class="k">return</span> <span class="n">embedding</span>
</span></pre></div>


            <div class="docstring"><p>Create sinusoidal timestep embeddings.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>t</strong>:  a 1-D Tensor of N indices, one per batch element.
These may be fractional.</li>
<li><strong>dim</strong>:  the dimension of the output.</li>
<li><strong>max_period</strong>:  controls the minimum frequency of the embeddings.</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>an (N, D) Tensor of positional embeddings.</p>
</blockquote>
</div>


                </section>
                <section id="RMSNorm">
                            <input id="RMSNorm-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">RMSNorm</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="RMSNorm-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RMSNorm"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RMSNorm-404"><a href="#RMSNorm-404"><span class="linenos">404</span></a><span class="k">class</span><span class="w"> </span><span class="nc">RMSNorm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="RMSNorm-405"><a href="#RMSNorm-405"><span class="linenos">405</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="RMSNorm-406"><a href="#RMSNorm-406"><span class="linenos">406</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="RMSNorm-407"><a href="#RMSNorm-407"><span class="linenos">407</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
</span><span id="RMSNorm-408"><a href="#RMSNorm-408"><span class="linenos">408</span></a>
</span><span id="RMSNorm-409"><a href="#RMSNorm-409"><span class="linenos">409</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="RMSNorm-410"><a href="#RMSNorm-410"><span class="linenos">410</span></a>        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="RMSNorm-411"><a href="#RMSNorm-411"><span class="linenos">411</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="RMSNorm-412"><a href="#RMSNorm-412"><span class="linenos">412</span></a>        <span class="n">rrms</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
</span><span id="RMSNorm-413"><a href="#RMSNorm-413"><span class="linenos">413</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">rrms</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x_dtype</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#RMSNorm.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#RMSNorm.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="RMSNorm.__init__" class="classattr">
                                        <input id="RMSNorm.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">RMSNorm</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span></span>)</span>

                <label class="view-source-button" for="RMSNorm.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RMSNorm.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RMSNorm.__init__-405"><a href="#RMSNorm.__init__-405"><span class="linenos">405</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="RMSNorm.__init__-406"><a href="#RMSNorm.__init__-406"><span class="linenos">406</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="RMSNorm.__init__-407"><a href="#RMSNorm.__init__-407"><span class="linenos">407</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="RMSNorm.scale" class="classattr">
                                <div class="attr variable">
            <span class="name">scale</span>

        
    </div>
    <a class="headerlink" href="#RMSNorm.scale"></a>
    
    

                            </div>
                            <div id="RMSNorm.forward" class="classattr">
                                        <input id="RMSNorm.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="RMSNorm.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RMSNorm.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RMSNorm.forward-409"><a href="#RMSNorm.forward-409"><span class="linenos">409</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="RMSNorm.forward-410"><a href="#RMSNorm.forward-410"><span class="linenos">410</span></a>        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="RMSNorm.forward-411"><a href="#RMSNorm.forward-411"><span class="linenos">411</span></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span><span id="RMSNorm.forward-412"><a href="#RMSNorm.forward-412"><span class="linenos">412</span></a>        <span class="n">rrms</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
</span><span id="RMSNorm.forward-413"><a href="#RMSNorm.forward-413"><span class="linenos">413</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">rrms</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x_dtype</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="QKNorm">
                            <input id="QKNorm-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">QKNorm</span><wbr>(<span class="base">torch.nn.modules.module.Module</span>):

                <label class="view-source-button" for="QKNorm-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#QKNorm"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="QKNorm-416"><a href="#QKNorm-416"><span class="linenos">416</span></a><span class="k">class</span><span class="w"> </span><span class="nc">QKNorm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span><span id="QKNorm-417"><a href="#QKNorm-417"><span class="linenos">417</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="QKNorm-418"><a href="#QKNorm-418"><span class="linenos">418</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="QKNorm-419"><a href="#QKNorm-419"><span class="linenos">419</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">query_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="QKNorm-420"><a href="#QKNorm-420"><span class="linenos">420</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">key_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="QKNorm-421"><a href="#QKNorm-421"><span class="linenos">421</span></a>
</span><span id="QKNorm-422"><a href="#QKNorm-422"><span class="linenos">422</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="QKNorm-423"><a href="#QKNorm-423"><span class="linenos">423</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_norm</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="QKNorm-424"><a href="#QKNorm-424"><span class="linenos">424</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_norm</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span><span id="QKNorm-425"><a href="#QKNorm-425"><span class="linenos">425</span></a>        <span class="k">return</span> <span class="n">q</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">k</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all neural network modules.</p>

<p>Your models should also subclass this class.</p>

<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>

<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call <code><a href="#QKNorm.to">to()</a></code>, etc.</p>

<div class="alert note">

<p>As per the example above, an <code><a href="#QKNorm.__init__">__init__()</a></code> call to the parent class
must be made before assignment on the child.</p>

</div>

<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>
</div>


                            <div id="QKNorm.__init__" class="classattr">
                                        <input id="QKNorm.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">QKNorm</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span></span>)</span>

                <label class="view-source-button" for="QKNorm.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#QKNorm.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="QKNorm.__init__-417"><a href="#QKNorm.__init__-417"><span class="linenos">417</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
</span><span id="QKNorm.__init__-418"><a href="#QKNorm.__init__-418"><span class="linenos">418</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="QKNorm.__init__-419"><a href="#QKNorm.__init__-419"><span class="linenos">419</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">query_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span><span id="QKNorm.__init__-420"><a href="#QKNorm.__init__-420"><span class="linenos">420</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">key_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</div>


                            </div>
                            <div id="QKNorm.query_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">query_norm</span>

        
    </div>
    <a class="headerlink" href="#QKNorm.query_norm"></a>
    
    

                            </div>
                            <div id="QKNorm.key_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">key_norm</span>

        
    </div>
    <a class="headerlink" href="#QKNorm.key_norm"></a>
    
    

                            </div>
                            <div id="QKNorm.forward" class="classattr">
                                        <input id="QKNorm.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="QKNorm.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#QKNorm.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="QKNorm.forward-422"><a href="#QKNorm.forward-422"><span class="linenos">422</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="QKNorm.forward-423"><a href="#QKNorm.forward-423"><span class="linenos">423</span></a>        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_norm</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span><span id="QKNorm.forward-424"><a href="#QKNorm.forward-424"><span class="linenos">424</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_norm</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span><span id="QKNorm.forward-425"><a href="#QKNorm.forward-425"><span class="linenos">425</span></a>        <span class="k">return</span> <span class="n">q</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">k</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Define the computation performed at every call.</p>

<p>Should be overridden by all subclasses.</p>

<div class="alert note">

<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>

</div>
</div>


                            </div>
                </section>
                <section id="attention">
                            <input id="attention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">attention</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">pe</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="attention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#attention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="attention-428"><a href="#attention-428"><span class="linenos">428</span></a><span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pe</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="attention-429"><a href="#attention-429"><span class="linenos">429</span></a>    <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">apply_rope</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</span><span id="attention-430"><a href="#attention-430"><span class="linenos">430</span></a>
</span><span id="attention-431"><a href="#attention-431"><span class="linenos">431</span></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span><span id="attention-432"><a href="#attention-432"><span class="linenos">432</span></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;B H L D -&gt; B L (H D)&quot;</span><span class="p">)</span>
</span><span id="attention-433"><a href="#attention-433"><span class="linenos">433</span></a>
</span><span id="attention-434"><a href="#attention-434"><span class="linenos">434</span></a>    <span class="k">return</span> <span class="n">x</span>
</span></pre></div>


    

                </section>
                <section id="rope">
                            <input id="rope-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">rope</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">pos</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>, </span><span class="param"><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span>, </span><span class="param"><span class="n">theta</span><span class="p">:</span> <span class="nb">int</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="rope-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#rope"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="rope-437"><a href="#rope-437"><span class="linenos">437</span></a><span class="k">def</span><span class="w"> </span><span class="nf">rope</span><span class="p">(</span><span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="rope-438"><a href="#rope-438"><span class="linenos">438</span></a>    <span class="k">assert</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
</span><span id="rope-439"><a href="#rope-439"><span class="linenos">439</span></a>    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span>
</span><span id="rope-440"><a href="#rope-440"><span class="linenos">440</span></a>    <span class="n">omega</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta</span><span class="o">**</span><span class="n">scale</span><span class="p">)</span>
</span><span id="rope-441"><a href="#rope-441"><span class="linenos">441</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;...n,d-&gt;...nd&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">omega</span><span class="p">)</span>
</span><span id="rope-442"><a href="#rope-442"><span class="linenos">442</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">out</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">out</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="rope-443"><a href="#rope-443"><span class="linenos">443</span></a>    <span class="n">out</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s2">&quot;b n d (i j) -&gt; b n d i j&quot;</span><span class="p">,</span> <span class="n">i</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">j</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="rope-444"><a href="#rope-444"><span class="linenos">444</span></a>    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></pre></div>


    

                </section>
                <section id="apply_rope">
                            <input id="apply_rope-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">apply_rope</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">xq</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">xk</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">freqs_cis</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="apply_rope-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#apply_rope"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="apply_rope-447"><a href="#apply_rope-447"><span class="linenos">447</span></a><span class="k">def</span><span class="w"> </span><span class="nf">apply_rope</span><span class="p">(</span><span class="n">xq</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">xk</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
</span><span id="apply_rope-448"><a href="#apply_rope-448"><span class="linenos">448</span></a>    <span class="n">xq_</span> <span class="o">=</span> <span class="n">xq</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xq</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="apply_rope-449"><a href="#apply_rope-449"><span class="linenos">449</span></a>    <span class="n">xk_</span> <span class="o">=</span> <span class="n">xk</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xk</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="apply_rope-450"><a href="#apply_rope-450"><span class="linenos">450</span></a>    <span class="n">xq_out</span> <span class="o">=</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">xq_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xq_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="apply_rope-451"><a href="#apply_rope-451"><span class="linenos">451</span></a>    <span class="n">xk_out</span> <span class="o">=</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">xk_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">xk_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="apply_rope-452"><a href="#apply_rope-452"><span class="linenos">452</span></a>    <span class="k">return</span> <span class="n">xq_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xq</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xq</span><span class="p">),</span> <span class="n">xk_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xk</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
</span></pre></div>


    

                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>